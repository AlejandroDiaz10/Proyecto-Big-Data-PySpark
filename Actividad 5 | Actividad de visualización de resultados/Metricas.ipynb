{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc93ef7d",
   "metadata": {},
   "source": [
    "# Actividad 4 | Métricas de calidad de resultados\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ec30f5",
   "metadata": {},
   "source": [
    "**MAESTRÍA EN INTELIGENCIA ARTIFICIAL APLICADA**\n",
    "\n",
    "**Curso: TC4034.10 - Análisis de grandes volúmenes de datos**\n",
    "\n",
    "Tecnológico de Monterrey\n",
    "\n",
    "* Dr. Iván Olmos Pineda\n",
    "* Mtra. Verónica Sandra Guzmán de Valle\n",
    "* Mtro. Alberto Daniel Salinas Montemayor\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc9a9d0",
   "metadata": {},
   "source": [
    "**Datos del alumno**\n",
    "\n",
    "|  NOMBRE COMPLETO                        |     MATRÍCULA     |\n",
    "| :-------------------------------------: |:-----------------:|\n",
    "| Emiliano Saucedo Arriola                |  A01659258        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91480e99",
   "metadata": {},
   "source": [
    "## 0. Inicialización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12334dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from functools import reduce\n",
    "from operator import and_\n",
    "\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, log1p\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92ec906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/emisaar/anaconda3/envs/pyspark_env/lib/python3.10/site-packages/pyspark'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c08099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileManager():\n",
    "    @staticmethod\n",
    "    def open_csv_file(file_path: str):\n",
    "        \"\"\"\n",
    "        Abre un archivo CSV usando PySpark con configuraciones optimizadas\n",
    "        \"\"\"\n",
    "        csv_df = spark.read.csv(\n",
    "            file_path,\n",
    "            header=True,\n",
    "            inferSchema=True,\n",
    "            multiLine=True,\n",
    "            escape=\"\\\"\",\n",
    "            quote=\"\\\"\"\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset original cargado: {csv_df.count():,} registros\")\n",
    "        csv_df.show(5, truncate=20)\n",
    "        return csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c8d42e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/06 18:56:02 WARN Utils: Your hostname, Emis-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.8 instead (on interface en0)\n",
      "25/06/06 18:56:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/06 18:56:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x15372d0c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ed1a46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emisaar/anaconda3/envs/pyspark_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/emisaar/.cache/kagglehub/datasets/machharavikiran/amazon-reviews/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "FILE_PATH = kagglehub.dataset_download(\"machharavikiran/amazon-reviews\")\n",
    "print(\"Path to dataset files:\", FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f79c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset original cargado: 6,906,564 registros\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|sentiment|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "|         US|   22873041|R3ARRMDEGED8RD|B00KJWQIIC|     335625766|Plemo 14-Inch Lap...|              PC|          5|            0|          0|   N|                Y|Pleasantly surprised|I was very surpri...| 2015-08-31|        1|\n",
      "|         US|   30088427| RQ28TSA020Y6J|B013ALA9LA|     671157305|TP-Link OnHub AC1...|              PC|          5|           24|         31|   N|                N|OnHub is a pretty...|I am a Google emp...| 2015-08-31|        1|\n",
      "|         US|   20329786| RUXJRZCT6953M|B00PML2GQ8|     982036237|AmazonBasics USB ...|              PC|          1|            2|          2|   N|                N|None of them work...|Bought cables in ...| 2015-08-31|        0|\n",
      "|         US|   14215710| R7EO0UO6BPB71|B001NS0OZ4|     576587596|Transcend P8 15-i...|              PC|          1|            0|          0|   N|                Y|just keep searching.|nope, cheap and slow| 2015-08-31|        0|\n",
      "|         US|   38264512|R39NJY2YJ1JFSV|B00AQMTND2|     964759214|Aleratec SATA Dat...|              PC|          5|            0|          0|   N|                Y|          Five Stars|Excellent! Great ...| 2015-08-31|        1|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar dataset (población P)\n",
    "df_reviews_P = FileManager.open_csv_file(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dcdaab",
   "metadata": {},
   "source": [
    "## 1.  Construcción de la muestra M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bba0c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir variables de caracterización\n",
    "characterization_variables = [\"star_rating\", \"verified_purchase\", \"vine\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf7f3c4",
   "metadata": {},
   "source": [
    "#### Análisis de la población P\n",
    "\n",
    "En este apartado se realiza un análisis exploratorio sobre el dataset completo (población P). El objetivo es comprender la distribución general de las variables de caracterización (`star_rating`, `verified_purchase`, `vine`) y cómo se combinan entre sí, para identificar patrones, sesgos y grupos minoritarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3ba0ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño total de la población P: 6,906,564 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Analizar distribución de la población\n",
    "total_count = df_reviews_P.count()\n",
    "print(f\"Tamaño total de la población P: {total_count:,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f427bb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribución de star_rating:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+\n",
      "|star_rating|  count|percentage|\n",
      "+-----------+-------+----------+\n",
      "|          5|4105687|     59.45|\n",
      "|          4|1168208|     16.91|\n",
      "|          1| 756857|     10.96|\n",
      "|          3| 513656|      7.44|\n",
      "|          2| 362156|      5.24|\n",
      "+-----------+-------+----------+\n",
      "\n",
      "\n",
      "Distribución de verified_purchase:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+----------+\n",
      "|verified_purchase|  count|percentage|\n",
      "+-----------------+-------+----------+\n",
      "|                Y|6047075|     87.56|\n",
      "|                N| 859489|     12.44|\n",
      "+-----------------+-------+----------+\n",
      "\n",
      "\n",
      "Distribución de vine:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------+\n",
      "|vine|  count|percentage|\n",
      "+----+-------+----------+\n",
      "|   N|6870336|     99.48|\n",
      "|   Y|  36228|      0.52|\n",
      "+----+-------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calcular distribución de cada variable individual\n",
    "for var in characterization_variables:\n",
    "    print(f\"\\nDistribución de {var}:\")\n",
    "    var_dist = df_reviews_P.groupBy(var).count() \\\n",
    "                .withColumn(\"percentage\", F.round(F.col(\"count\") * 100.0 / total_count, 2)) \\\n",
    "                .orderBy(\"count\", ascending=False)\n",
    "    var_dist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e45c1",
   "metadata": {},
   "source": [
    "**Observaciones**\n",
    "\n",
    "***Distribución de calificaciones (`star_rating`)***\n",
    "* El 59.45% de las reseñas son de 5 estrellas, mostrando un fuerte sesgo positivo.\n",
    "* Las reseñas con 1 estrella representan el 10.96%, mientras que las reseñas intermedias (2–4 estrellas) representan el resto.\n",
    "\n",
    "***Distribución de compras verificadas (`verified_purchase`)***\n",
    "* La gran mayoría (87.56%) proviene de usuarios con compra verificada, indicando que la base de datos tiene un fuerte enfoque en opiniones genuinas.\n",
    "\n",
    "***Distribución del programa Vine (`vine`)***\n",
    "* Solo el 52% de las reseñas provienen del programa Vine (usuarios seleccionados para reseñar productos), mientras que el 99.48% no pertenece a este grupo, haciéndolo un segmento muy minoritario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e568c864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución conjunta de ['star_rating', 'verified_purchase', 'vine']:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+----+-------+-----------+\n",
      "|star_rating|verified_purchase|vine|  count|probability|\n",
      "+-----------+-----------------+----+-------+-----------+\n",
      "|          5|                Y|   N|3679909|   0.532813|\n",
      "|          4|                Y|   N|1019728|   0.147646|\n",
      "|          1|                Y|   N| 603371|   0.087362|\n",
      "|          3|                Y|   N| 443364|   0.064195|\n",
      "|          5|                N|   N| 410073|   0.059374|\n",
      "|          2|                Y|   N| 300544|   0.043516|\n",
      "|          1|                N|   N| 152779|   0.022121|\n",
      "|          4|                N|   N| 135197|   0.019575|\n",
      "|          3|                N|   N|  65398|   0.009469|\n",
      "|          2|                N|   N|  59973|   0.008683|\n",
      "|          5|                N|   Y|  15604|   0.002259|\n",
      "|          4|                N|   Y|  13240|   0.001917|\n",
      "|          3|                N|   Y|   4886|    7.07E-4|\n",
      "|          2|                N|   Y|   1634|    2.37E-4|\n",
      "|          1|                N|   Y|    705|    1.02E-4|\n",
      "|          5|                Y|   Y|    101|     1.5E-5|\n",
      "|          4|                Y|   Y|     43|     6.0E-6|\n",
      "|          3|                Y|   Y|      8|     1.0E-6|\n",
      "|          2|                Y|   Y|      5|     1.0E-6|\n",
      "|          1|                Y|   Y|      2|        0.0|\n",
      "+-----------+-----------------+----+-------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calcular distribución conjunta\n",
    "print(f\"Distribución conjunta de {characterization_variables}:\")\n",
    "population_dist = df_reviews_P.groupBy(characterization_variables).count() \\\n",
    "                .withColumn(\"probability\", F.round(F.col(\"count\") / total_count, 6)) \\\n",
    "                .orderBy(\"probability\", ascending=False)\n",
    "population_dist.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285ee96f",
   "metadata": {},
   "source": [
    "**Observaciones**\n",
    "* Más de la mitad son reseñas de 5 estrellas, verificadas y sin Vine (53.28%).\n",
    "* Los grupos minoritarios, como reseñas de Vine con calificaciones bajas (1-3), tienen probabilidades extremadamente pequeñas.\n",
    "* Esto confirma que la población está altamente desbalanceada, con algunos grupos muy dominantes y otros extremadamente escasos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd449b7",
   "metadata": {},
   "source": [
    "#### Filtrado de los estratos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "402f1a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estratos identificados: 15\n",
      "Probabilidad mínima requerida: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Identificar estratos relevantes (combinaciones con suficiente probabilidad)\n",
    "min_probability = 0.0001\n",
    "relevant_strata = population_dist.filter(F.col(\"probability\") >= min_probability).collect()\n",
    "\n",
    "print(f\"Estratos identificados: {len(relevant_strata)}\")\n",
    "print(f\"Probabilidad mínima requerida: {min_probability:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0060d",
   "metadata": {},
   "source": [
    "**Observaciones**\n",
    "\n",
    "Al analizar todas las combinaciones posibles de las variables de caracterización, se seleccionaron solo aquellos estratos (15 identificados) que tienen una probabilidad mínima (mayor o igual a 0.0001) de ocurrencia en la población. Esto asegura trabajar solo con grupos significativos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d5dd20",
   "metadata": {},
   "source": [
    "#### Fracción de muestreo\n",
    "A continuación, se construye un diccionario en el que se calcula la fracción de muestreo que se aplicará a cada estrato relevante para construir la muestra representativa M. Para mantener la diversidad y la representatividad de los datos, se optó por tomar el 10% de cada grupo, con un pequeño ajuste (~12%) para compensar la variabilidad aleatoria del muestreo y evitar que algunos grupos minoritarios terminen demasiado reducidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "596694e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estrato R5_VPY_VN: 3,679,909 registros, fracción: 0.120\n",
      "Estrato R4_VPY_VN: 1,019,728 registros, fracción: 0.120\n",
      "Estrato R1_VPY_VN: 603,371 registros, fracción: 0.120\n",
      "Estrato R3_VPY_VN: 443,364 registros, fracción: 0.120\n",
      "Estrato R5_VPN_VN: 410,073 registros, fracción: 0.120\n",
      "Estrato R2_VPY_VN: 300,544 registros, fracción: 0.120\n",
      "Estrato R1_VPN_VN: 152,779 registros, fracción: 0.120\n",
      "Estrato R4_VPN_VN: 135,197 registros, fracción: 0.120\n",
      "Estrato R3_VPN_VN: 65,398 registros, fracción: 0.120\n",
      "Estrato R2_VPN_VN: 59,973 registros, fracción: 0.120\n",
      "Estrato R5_VPN_VY: 15,604 registros, fracción: 0.120\n",
      "Estrato R4_VPN_VY: 13,240 registros, fracción: 0.120\n",
      "Estrato R3_VPN_VY: 4,886 registros, fracción: 0.120\n",
      "Estrato R2_VPN_VY: 1,634 registros, fracción: 0.120\n",
      "Estrato R1_VPN_VY: 705 registros, fracción: 0.120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Crear fracciones de muestreo para cada estrato\n",
    "strata_fractions = {}\n",
    "sample_fraction = 0.1\n",
    "total_population = df_reviews_P.count()\n",
    "\n",
    "for row in relevant_strata:\n",
    "    # Crear clave única para el estrato\n",
    "    star_label = f\"R{row['star_rating']}\"\n",
    "    vp_label = f\"VP{'Y' if row['verified_purchase'] == 'Y' else 'N'}\"\n",
    "    vine_label = f\"V{'Y' if row['vine'] == 'Y' else 'N'}\"\n",
    "\n",
    "    strata_key = f\"{star_label}_{vp_label}_{vine_label}\"\n",
    "    \n",
    "    # Calcular fracción de muestreo para mantener representatividad\n",
    "    strata_fraction = min(sample_fraction * 1.2, 0.5)  # Ligeramente mayor para compensar variabilidad\n",
    "    strata_fractions[strata_key] = strata_fraction\n",
    "    \n",
    "    print(f\"Estrato {strata_key}: {row['count']:,} registros, fracción: {strata_fraction:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e1e4a",
   "metadata": {},
   "source": [
    "#### Construcción de M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81fc7c",
   "metadata": {},
   "source": [
    "Para cada estrato, se filtran los registros y se toma una muestra utilizando la fracción previamente calculada. Este muestreo se realiza con una semilla fija (seed=42) para garantizar la reproducibilidad. Además, se asegura que no haya reemplazo (`withReplacement=False`), lo que significa que cada registro puede aparecer solo una vez en la muestra final.\n",
    "\n",
    "Fuentes consultadas:\n",
    "* https://www.geeksforgeeks.org/pyspark-random-sample-with-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc4d3f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestreados 442,073 de 3,679,909 en estrato R5_VPY_VN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestreados 122,773 de 1,019,728 en estrato R4_VPY_VN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestreados 72,444 de 603,371 en estrato R1_VPY_VN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestreados 53,285 de 443,364 en estrato R3_VPY_VN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestreados 49,191 de 410,073 en estrato R5_VPN_VN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestreados 36,089 de 300,544 en estrato R2_VPY_VN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestreados 18,316 de 152,779 en estrato R1_VPN_VN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestreados 16,171 de 135,197 en estrato R4_VPN_VN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestreados 7,838 de 65,398 en estrato R3_VPN_VN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestreados 7,208 de 59,973 en estrato R2_VPN_VN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestreados 1,936 de 15,604 en estrato R5_VPN_VY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestreados 1,674 de 13,240 en estrato R4_VPN_VY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestreados 606 de 4,886 en estrato R3_VPN_VY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestreados 213 de 1,634 en estrato R2_VPN_VY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 119:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestreados 83 de 705 en estrato R1_VPN_VY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Aplicar muestreo estratificado\n",
    "sample_parts = []\n",
    "\n",
    "for row in relevant_strata:\n",
    "    # Filtrar datos del estrato actual\n",
    "    conditions = [F.col(var) == row[var] for var in characterization_variables]\n",
    "    combined_condition = conditions[0]\n",
    "    for condition in conditions[1:]:\n",
    "        combined_condition = combined_condition & condition\n",
    "    \n",
    "    strata_data = df_reviews_P.filter(combined_condition)\n",
    "    \n",
    "    # Crear clave única para el estrato\n",
    "    star_label = f\"R{row['star_rating']}\"\n",
    "    vp_label = f\"VP{'Y' if row['verified_purchase'] == 'Y' else 'N'}\"\n",
    "    vine_label = f\"V{'Y' if row['vine'] == 'Y' else 'N'}\"\n",
    "\n",
    "    strata_key = f\"{star_label}_{vp_label}_{vine_label}\"\n",
    "    fraction = strata_fractions[strata_key]\n",
    "    \n",
    "    # Aplicar muestreo con semilla fija para reproducibilidad\n",
    "    sampled_strata = strata_data.sample(withReplacement=False, fraction=fraction, seed=42)\n",
    "    sample_parts.append(sampled_strata)\n",
    "    \n",
    "    print(f\"Muestreados {sampled_strata.count():,} de {strata_data.count():,} en estrato {strata_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e39bd",
   "metadata": {},
   "source": [
    "Posteriormente, se unen todas las submuestras recolectadas de los distintos estratos para construir la muestra representativa M. Esto se logra concatenando cada subconjunto, asegurando que todos los grupos seleccionados formen parte del conjunto final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da13fec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 122:=============================>                          (8 + 7) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MUESTRA M CREADA ===\n",
      "Tamaño de la muestra M: 829,900 registros\n",
      "Fracción real obtenida: 12.0%\n",
      "Fracción objetivo: 10.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Unir todas las partes para formar la muestra M\n",
    "sample_M = sample_parts[0]\n",
    "for part in sample_parts[1:]:\n",
    "    sample_M = sample_M.union(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be0de416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 128:=============================>                          (8 + 7) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Población P: 6,906,564 registros\n",
      "Muestra M: 829,900 registros\n",
      "Factor de muestreo: 12.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Validar que la muestra M sea representativa de la población P\n",
    "# Comparar distribuciones\n",
    "pop_total = df_reviews_P.count()\n",
    "sample_total = sample_M.count()\n",
    "\n",
    "print(f\"Población P: {pop_total:,} registros\")\n",
    "print(f\"Muestra M: {sample_total:,} registros\")\n",
    "print(f\"Factor de muestreo: {sample_total/pop_total:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b212cd2",
   "metadata": {},
   "source": [
    "**Observaciones**\n",
    "\n",
    "Se obtuvo una muestra M de 829,900 registros, equivalente al 12% de la población original.\n",
    "\n",
    "Aunque el objetivo inicial era obtener aproximadamente el 10%, el ajuste aplicado permitió compensar la variabilidad del proceso de muestreo y asegurar que los estratos más pequeños quedaran representados adecuadamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f5c50ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparación para star_rating ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------------+--------------------+\n",
      "|star_rating|pop_percentage|sample_percentage|          difference|\n",
      "+-----------+--------------+-----------------+--------------------+\n",
      "|          1|         10.96|            10.95|0.010000000000001563|\n",
      "|          2|          5.24|             5.24|                 0.0|\n",
      "|          3|          7.44|             7.44|                 0.0|\n",
      "|          4|         16.91|            16.94|0.030000000000001137|\n",
      "|          5|         59.45|            59.43|0.020000000000003126|\n",
      "+-----------+--------------+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diferencia promedio en star_rating: 0.01 puntos porcentuales\n",
      "\n",
      "--- Comparación para verified_purchase ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+-----------------+----------+\n",
      "|verified_purchase|pop_percentage|sample_percentage|difference|\n",
      "+-----------------+--------------+-----------------+----------+\n",
      "|                N|         12.44|            12.44|       0.0|\n",
      "|                Y|         87.56|            87.56|       0.0|\n",
      "+-----------------+--------------+-----------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diferencia promedio en verified_purchase: 0.00 puntos porcentuales\n",
      "\n",
      "--- Comparación para vine ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-----------------+--------------------+\n",
      "|vine|pop_percentage|sample_percentage|          difference|\n",
      "+----+--------------+-----------------+--------------------+\n",
      "|   N|         99.48|            99.46|0.020000000000010232|\n",
      "|   Y|          0.52|             0.54|0.020000000000000018|\n",
      "+----+--------------+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 181:==========================>                             (7 + 8) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diferencia promedio en vine: 0.02 puntos porcentuales\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Comparar distribución de cada variable\n",
    "for var in characterization_variables:\n",
    "    print(f\"\\n--- Comparación para {var} ---\")\n",
    "    \n",
    "    # Distribución en población\n",
    "    pop_dist = df_reviews_P.groupBy(var).count() \\\n",
    "                            .withColumn(\"pop_percentage\", F.round(F.col(\"count\") * 100.0 / pop_total, 2)) \\\n",
    "                            .select(var, F.col(\"pop_percentage\"))\n",
    "    \n",
    "    # Distribución en muestra\n",
    "    sample_dist = sample_M.groupBy(var).count() \\\n",
    "                            .withColumn(\"sample_percentage\", F.round(F.col(\"count\") * 100.0 / sample_total, 2)) \\\n",
    "                            .select(var, F.col(\"sample_percentage\"))\n",
    "    \n",
    "    # Combinar para comparar\n",
    "    comparison = pop_dist.join(sample_dist, on=var, how=\"outer\") \\\n",
    "                        .fillna(0) \\\n",
    "                        .withColumn(\"difference\", \n",
    "                                    F.abs(F.col(\"pop_percentage\") - F.col(\"sample_percentage\"))) \\\n",
    "                        .orderBy(var)\n",
    "    \n",
    "    comparison.show()\n",
    "    \n",
    "    # Calcular diferencia promedio\n",
    "    avg_diff = comparison.agg(F.avg(\"difference\")).collect()[0][0]\n",
    "    print(f\"Diferencia promedio en {var}: {avg_diff:.2f} puntos porcentuales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df78039e",
   "metadata": {},
   "source": [
    "**Observaciones**\n",
    "\n",
    "Con esta validación, nos aseguramos que el muestreo estratificado preservó las proporciones de la población, asegurando que la muestra sea representativa y fiable. Los resultados muestran que las diferencias promedio entre la población original y la muestra son casi nulas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc93bc",
   "metadata": {},
   "source": [
    "#### Particionamiento\n",
    "\n",
    "A continuación, se crean las particiones Mi a partir de la muestra M, filtrando los datos para cada combinación de las variables de caracterización (star_rating, verified_purchase, vine). Para cada combinación, se construye dinámicamente una condición de filtrado que selecciona únicamente los registros que coinciden simultáneamente con esos valores específicos. Solo se conservan las particiones que tienen al menos 100 registros para asegurar que sean suficientemente representativas y útiles para construir más adelante los conjuntos de entrenamiento y prueba. Cada partición se guarda con un nombre descriptivo (como R5_VPY_VN), lo que permite identificar rápidamente a qué grupo pertenece. Al final, se obtiene un conjunto organizado de subconjuntos válidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caea6009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 197:=============================>                          (8 + 7) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinaciones encontradas en M: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Identificar todas las combinaciones existentes en M\n",
    "combinations = sample_M.select(characterization_variables).distinct().collect()\n",
    "\n",
    "print(f\"Combinaciones encontradas en M: {len(combinations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b8c9d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición R5_VPY_VN: 442,073 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición R4_VPY_VN: 122,773 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición R1_VPY_VN: 72,444 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición R3_VPY_VN: 53,285 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición R5_VPN_VN: 49,191 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición R2_VPY_VN: 36,089 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición R1_VPN_VN: 18,316 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición R4_VPN_VN: 16,171 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición R3_VPN_VN: 7,838 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición R2_VPN_VN: 7,208 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición R5_VPN_VY: 1,936 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición R4_VPN_VY: 1,674 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición R3_VPN_VY: 606 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición R2_VPN_VY: 213 registros\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 242:=============================>                          (8 + 7) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluida combinación (star_rating=1, verified_purchase=N, vine=Y): solo 83 registros (< 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Crear particiones Mi\n",
    "partitions_Mi = {}\n",
    "total_partitioned = 0\n",
    "min_partition_size = 100\n",
    "\n",
    "for row in combinations:\n",
    "    # Crear condiciones de filtrado\n",
    "    combined_condition = reduce(and_, [F.col(var) == row[var] for var in characterization_variables])\n",
    "    \n",
    "    # Filtrar para crear la partición\n",
    "    partition = sample_M.filter(combined_condition)\n",
    "    partition_size = partition.count()\n",
    "    \n",
    "    # Solo incluir si cumple con el tamaño mínimo\n",
    "    if partition_size >= min_partition_size:\n",
    "        # Crear nombre descriptivo para la partición\n",
    "        star_label = f\"R{row['star_rating']}\"\n",
    "        vp_label = f\"VP{'Y' if row['verified_purchase'] == 'Y' else 'N'}\"\n",
    "        vine_label = f\"V{'Y' if row['vine'] == 'Y' else 'N'}\"\n",
    "\n",
    "        partition_key = f\"{star_label}_{vp_label}_{vine_label}\"\n",
    "        partitions_Mi[partition_key] = partition\n",
    "        total_partitioned += partition_size\n",
    "        \n",
    "        print(f\"Partición {partition_key}: {partition_size:,} registros\")\n",
    "    else:\n",
    "        key_values = [f\"{var}={row[var]}\" for var in characterization_variables]\n",
    "        combination_str = \", \".join(key_values)\n",
    "        print(f\"Excluida combinación ({combination_str}): solo {partition_size} registros (< {min_partition_size})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8aece47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 245:=============================>                          (8 + 7) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESUMEN DE PARTICIONAMIENTO ===\n",
      "Particiones Mi creadas: 14\n",
      "Registros en M: 829,900\n",
      "Registros particionados: 829,817\n",
      "Cobertura: 99.9900%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 3. Verificar que la unión de particiones = M\n",
    "sample_M_size = sample_M.count()\n",
    "coverage = total_partitioned / sample_M_size\n",
    "\n",
    "print(f\"\\n=== RESUMEN DE PARTICIONAMIENTO ===\")\n",
    "print(f\"Particiones Mi creadas: {len(partitions_Mi)}\")\n",
    "print(f\"Registros en M: {sample_M_size:,}\")\n",
    "print(f\"Registros particionados: {total_partitioned:,}\")\n",
    "print(f'Cobertura: {coverage * 100:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d15bf",
   "metadata": {},
   "source": [
    "**Observaciones**\n",
    "\n",
    "Al ejecutar el particionamiento, se crearon 14 particiones Mi, que en conjunto cubren los registros de la muestra M (829,900 registros), donde 829,817 fueron particionados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a405c",
   "metadata": {},
   "source": [
    "## 2. Construcción del conjunto de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aeb9e6",
   "metadata": {},
   "source": [
    "En este apartado se toma cada partición Mi generada en el paso anterior y se divide en dos subconjuntos: Entrenamiento (Train) y Prueba (Test).\n",
    "\n",
    "Antes de separar los datos, se analiza el tamaño de la clase minoritaria en todas las particiones para determinar el porcentaje adecuado de división.\n",
    "Dependiendo del tamaño de la clase minoritaria, se elige un split (train-test):\n",
    "* 70%-30% -> si tiene mayor o igual a 100 ejemplos\n",
    "* 75%-25% -> si tiene entre 60 y 99 ejemplos\n",
    "* 80%-20% -> si tiene menos de 60 ejemplos\n",
    "\n",
    "Se decidió realizar de estar manera para asegurar que los grupos más pequeños tengan al menos 20 ejemplos por clase en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e19752de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Analizar las particiones para identificar la clase minoritaria\n",
    "min_class_size = float('inf')\n",
    "target_col = \"sentiment\"\n",
    "\n",
    "for partition_key, partition_df in partitions_Mi.items():\n",
    "    class_counts = partition_df.groupBy(target_col).count().collect()\n",
    "    for row in class_counts:\n",
    "        if row['count'] < min_class_size:\n",
    "            min_class_size = row['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b904b287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio óptimo calculado: 70% train / 30% test\n"
     ]
    }
   ],
   "source": [
    "# Determinar el ratio óptimo basado en la clase minoritaria\n",
    "if min_class_size >= 100:\n",
    "    test_ratio = 0.3  # 70-30\n",
    "elif min_class_size >= 60:\n",
    "    test_ratio = 0.25  # 75-25\n",
    "else:\n",
    "    test_ratio = 0.2  # 80-20\n",
    "\n",
    "train_ratio = 1.0 - test_ratio\n",
    "\n",
    "print(f\"Ratio óptimo calculado: {train_ratio:.0%} train / {test_ratio:.0%} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252fa27",
   "metadata": {},
   "source": [
    "#### Separación de datos\n",
    "\n",
    "La separación se hace a partir de un muestreo estratificado para mantener la misma proporción de clases (`sentiment`) tanto en entrenamiento como en prueba, evitando sesgos.\n",
    "\n",
    "Primero se toma el conjunto de entrenamiento usando `sampleBy`, y luego se obtiene el conjunto de prueba tomando los registros restantes con `left_anti`. Con esto se busca evitar coincidencias entre ambos conjuntos, garantizando que los datos de prueba sean completamente independientes.\n",
    "\n",
    "Cabe destacar que, para cada partición, se realiza un guardado incremental en disco, lo que permite reanudar el proceso en caso de interrupciones sin recalcular todo. También, para reducir el tiempo de procesamiento, se optó por implementar procesamiento paralelo y persistencia en memoria.\n",
    "\n",
    "Fuentes consultadas:\n",
    "\n",
    "https://www.geeksforgeeks.org/pyspark-sampleby-using-multiple-columns/\n",
    "\n",
    "https://iomete.com/resources/reference/pyspark/pyspark-join\n",
    "\n",
    "https://medium.com/@shuklaprashant9264/pyspark-persist-2aa92478081d\n",
    "\n",
    "https://medium.com/@anupchakole/understanding-threadpoolexecutor-2eed095d21aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4156d218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saltando partición R4_VPY_VN (ya existe)Saltando partición R5_VPY_VN (ya existe)\n",
      "\n",
      "Saltando partición R3_VPY_VN (ya existe)\n",
      "Saltando partición R1_VPY_VN (ya existe)\n",
      "Saltando partición R1_VPN_VN (ya existe)\n",
      "Saltando partición R5_VPN_VN (ya existe)\n",
      "Saltando partición R4_VPN_VN (ya existe)\n",
      "Saltando partición R2_VPN_VN (ya existe)\n",
      "Saltando partición R2_VPY_VN (ya existe)\n",
      "Saltando partición R3_VPN_VY (ya existe)\n",
      "Saltando partición R5_VPN_VY (ya existe)\n",
      "Saltando partición R4_VPN_VY (ya existe)\n",
      "Saltando partición R3_VPN_VN (ya existe)\n",
      "Saltando partición R2_VPN_VY (ya existe)\n"
     ]
    }
   ],
   "source": [
    "# Guardar los resultados de la partición\n",
    "output_dir = \"outputs/stratified_split\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "log_path = os.path.join(output_dir, \"split_summary.csv\")\n",
    "\n",
    "# Inicializar log si no existe\n",
    "if not os.path.exists(log_path):\n",
    "    with open(log_path, \"w\") as log_file:\n",
    "        log_file.write(\"partition,train_size,test_size,total_size\\n\")\n",
    "\n",
    "def process_partition(partition_key, partition_df):\n",
    "    train_path = f\"{output_dir}/train_{partition_key}\"\n",
    "    test_path = f\"{output_dir}/test_{partition_key}\"\n",
    "\n",
    "    if os.path.exists(train_path) and os.path.exists(test_path):\n",
    "        print(f\"Saltando partición {partition_key} (ya existe)\")\n",
    "        return\n",
    "\n",
    "    print(f\"Procesando partición {partition_key}...\")\n",
    "\n",
    "    # Persistir para evitar recálculo\n",
    "    partition_df = partition_df.persist()\n",
    "\n",
    "    class_counts = partition_df.groupBy(target_col).count().collect()\n",
    "    fractions = {row[target_col]: train_ratio for row in class_counts}\n",
    "\n",
    "    train_df = partition_df.sampleBy(target_col, fractions, seed=42).persist()\n",
    "    test_df = partition_df.join(train_df, on=partition_df.columns, how='left_anti').persist()\n",
    "\n",
    "    # Guardar en local\n",
    "    train_df.write.mode(\"overwrite\").parquet(train_path)\n",
    "    test_df.write.mode(\"overwrite\").parquet(test_path)\n",
    "\n",
    "    # Calcular tamaños solo una vez (ya persistidos)\n",
    "    train_size = train_df.count()\n",
    "    test_size = test_df.count()\n",
    "    total_size = train_size + test_size\n",
    "\n",
    "    print(f\"Train: {train_size} registros ({train_size/total_size:.1%})\")\n",
    "    print(f\"Test: {test_size} registros ({test_size/total_size:.1%})\")\n",
    "\n",
    "    # Registrar en log CSV\n",
    "    with open(log_path, \"a\") as log_file:\n",
    "        log_file.write(f\"{partition_key},{train_size},{test_size},{total_size}\\n\")\n",
    "\n",
    "    # Liberar memoria\n",
    "    partition_df.unpersist()\n",
    "    train_df.unpersist()\n",
    "    test_df.unpersist()\n",
    "\n",
    "# ⚙ Configura cuántas particiones en paralelo\n",
    "max_workers = 4\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = []\n",
    "    for key, df in partitions_Mi.items():\n",
    "        futures.append(executor.submit(process_partition, key, df))\n",
    "\n",
    "    # Esperar a que todas terminen\n",
    "    for future in futures:\n",
    "        future.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcacb67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición R2_VPN_VY: 133 train, 80 test\n",
      "Partición R3_VPN_VN: 5445 train, 2393 test\n",
      "Partición R5_VPY_VN: 309248 train, 132825 test\n",
      "Partición R1_VPN_VN: 12896 train, 5420 test\n",
      "Partición R5_VPN_VN: 34505 train, 14686 test\n",
      "Partición R1_VPY_VN: 50774 train, 21670 test\n",
      "Partición R4_VPN_VY: 1187 train, 487 test\n",
      "Partición R3_VPY_VN: 37301 train, 15984 test\n",
      "Partición R2_VPN_VN: 5043 train, 2165 test\n",
      "Partición R3_VPN_VY: 439 train, 167 test\n",
      "Partición R4_VPY_VN: 86077 train, 36696 test\n",
      "Partición R5_VPN_VY: 1367 train, 569 test\n",
      "Partición R4_VPN_VN: 11422 train, 4749 test\n",
      "Partición R2_VPY_VN: 25322 train, 10767 test\n"
     ]
    }
   ],
   "source": [
    "# Cargar los conjuntos de entrenamiento y prueba guardados\n",
    "output_dir = \"outputs/stratified_split\"\n",
    "train_sets = {}\n",
    "test_sets = {}\n",
    "\n",
    "# Buscar todos los archivos parquet guardados\n",
    "for filename in os.listdir(output_dir):\n",
    "    if filename.startswith(\"train_\"):\n",
    "        partition_key = filename[len(\"train_\"):]  # solo quitamos el prefijo\n",
    "        \n",
    "        train_path = f\"{output_dir}/{filename}\"\n",
    "        test_path = f\"{output_dir}/test_{partition_key}\"\n",
    "\n",
    "        try:\n",
    "            train_df = spark.read.parquet(train_path)\n",
    "            test_df = spark.read.parquet(test_path)\n",
    "            \n",
    "            train_sets[partition_key] = train_df\n",
    "            test_sets[partition_key] = test_df\n",
    "\n",
    "            print(f\"Partición {partition_key}: \"\n",
    "                  f\"{train_df.count()} train, {test_df.count()} test\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar partición {partition_key}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "334fb6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESUMEN FINAL DE DIVISIÓN ===\n",
      "Tamaño total de entrenamiento: 581,159 registros\n",
      "Tamaño total de prueba: 248,658 registros\n",
      "Suma total de registros: 829,817 registros\n",
      "Tamaño total original: 829,817 registros\n",
      "Fracción de entrenamiento: 70.0%\n",
      "Fracción de prueba: 30.0%\n"
     ]
    }
   ],
   "source": [
    "log_df = pd.read_csv(\"outputs/stratified_split/split_summary.csv\")\n",
    "total_train = log_df[\"train_size\"].sum()\n",
    "total_test = log_df[\"test_size\"].sum()\n",
    "total_original = log_df[\"total_size\"].sum()\n",
    "\n",
    "print(f\"\\n=== RESUMEN FINAL DE DIVISIÓN ===\")\n",
    "print(f\"Tamaño total de entrenamiento: {total_train:,} registros\")\n",
    "print(f\"Tamaño total de prueba: {total_test:,} registros\")\n",
    "print(f\"Suma total de registros: {total_train + total_test:,} registros\")\n",
    "print(f\"Tamaño total original: {total_original:,} registros\")\n",
    "print(f\"Fracción de entrenamiento: {total_train / total_original:.1%}\")\n",
    "print(f\"Fracción de prueba: {total_test / total_original:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f5fee1",
   "metadata": {},
   "source": [
    "**Observaciones**\n",
    "\n",
    "Se puede apreciar que la suma de los tamaños de los conjuntos de train y test coinciden con el tamaño original de cada partición Mi. Lo anterior, permite descartar la posibilidad de valores faltantes o duplicados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d47bbe2",
   "metadata": {},
   "source": [
    "## 3. Selección de métricas para medir calidad de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c727377",
   "metadata": {},
   "source": [
    "En este apartado se definen y justifican las métricas que se utilizarán para evaluar la calidad de los modelos construidos. Elegir las métricas adecuadas es fundamental porque nos permite entender no solo qué tan bien se ajusta el modelo a los datos, sino que también cómo se comporta frente a nuevos casos, qué tipo de errores comete y cómo balancea entre distintas prioridades (como precisión vs. sensibilidad).\n",
    "\n",
    "#### Contexto: Amazon Reviews\n",
    "El conjunto de datos contiene reseñas de productos publicadas en Amazon, centradas en la categoría electrónica. Cada registro incluye variables relevantes como la calificación (`star_rating`), si la compra fue verificada (`verified_purchase`), si participó en el programa Vine (`vine`) y una etiqueta derivada de sentimiento (`sentiment`). El objetivo es construir modelos que puedan clasificar correctamente el sentimiento de las reseñas. \n",
    "\n",
    "Dado que se trata de un problema de clasificación binaria, y considerando el tamaño y naturaleza del dataset (grandes volúmenes y con clases desbalanceadas), es necesario usar métricas robustas y que reflejen distintos aspectos del rendimiento del modelo.\n",
    "\n",
    "#### Selección de métricas\n",
    "\n",
    "Con base en el entendimiento del problema, se consideró el uso de las siguientes métricas:\n",
    "\n",
    "**Accuracy:** Mide el porcentaje total de predicciones correctas (ya sean positivas o negativas). En nuestro caso, nos da una visión general de cuántas reseñas de sentimiento fueron clasificadas correctamente. Sin embargo, dada la naturaleza del desbalanceo de clases, no podemos depender solo de esta métrica, porque un modelo que siempre prediga “positivo” podría tener alta precisión, pero no ser útil para detectar las negativas.\n",
    "\n",
    "**Precision:** De todas las reseñas que el modelo predijo como positivas, indica cuántas realmente lo son. Esto es relevante porque, si queremos usar el modelo para destacar reseñas confiables o promocionar productos con buena reputación, es importante evitar falsos positivos, ya que no queremos etiquetar como positiva una reseña que en realidad es negativa. Lo anterior podría distorcionar el análisis comercial.\n",
    "\n",
    "**Recall:** De todas las reseñas que realmente son positivas, mide cuántas logró capturar el modelo. Esto es fundamental si queremos capturar la mayor cantidad posible de reseñas positivas para recomendaciones.\n",
    "\n",
    "**F1-score:** Combina precision y recall, balanceando ambos aspectos. En este caso, es clave porque trabajamos con clases desbalanceadas (hay muchas más reseñas positivas que negativas), y necesitamos una métrica que penalice modelos que solo optimizan una dimensión (por ejemplo, mucha precisión pero poco recall).\n",
    "\n",
    "**AUC-ROC (Área bajo la curva ROC):** Evalúa la capacidad del modelo para distinguir correctamente entre reseñas positivas y negativas, considerando todos los umbrales posibles. Esto es útil para tener una visión completa del poder discriminativo del modelo, más allá de una decisión binaria fija.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e9b49",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento de Modelos de Aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef7b58",
   "metadata": {},
   "source": [
    "### Preprocesamiento de los datos\n",
    "\n",
    ">Nota: Se reutilizaron elementos de la Actividad 3 | Aprendizaje supervisado y no supervisado para llevar a cabo el procesamiento de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff87b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, columns_for_modeling):\n",
    "        self.columns = columns_for_modeling\n",
    "        self.indexed_columns = []\n",
    "\n",
    "    def merge_partitions(self, partition_dict: dict) -> DataFrame:\n",
    "        return reduce(lambda df1, df2: df1.unionByName(df2), partition_dict.values())\n",
    "\n",
    "    def drop_nulls(self, df: DataFrame) -> DataFrame:\n",
    "        initial_count = df.count()\n",
    "        print(f\"Total de registros antes de eliminar nulos: {initial_count}\")\n",
    "\n",
    "        # Calcular cuántos nulos hay por columna relevante\n",
    "        for col_name in self.columns:\n",
    "            null_count = df.filter(F.col(col_name).isNull()).count()\n",
    "            if null_count > 0:\n",
    "                print(f\"  Columna '{col_name}' tiene {null_count} valores nulos\")\n",
    "        \n",
    "        # Eliminar filas con nulos en columnas clave\n",
    "        df_clean = df.dropna(subset=self.columns)\n",
    "        final_count = df_clean.count()\n",
    "\n",
    "        print(f\"Total de registros después de eliminar nulos: {final_count}\")\n",
    "        print(f\"Se eliminaron {initial_count - final_count} registros ({(initial_count - final_count) / initial_count:.2%})\\n\")\n",
    "        \n",
    "        return df_clean\n",
    "\n",
    "    # def cast_types(self, df: DataFrame) -> DataFrame:\n",
    "    #     numeric_cols = [\"star_rating\", \"helpful_votes\", \"total_votes\", \"sentiment\"]\n",
    "    #     for col_name in numeric_cols:\n",
    "    #         df = df.withColumn(col_name, col(col_name).cast(IntegerType()))\n",
    "    #     return df\n",
    "\n",
    "    def remove_outliers(self, df: DataFrame, columns: list, quantile_error=0.01) -> DataFrame:\n",
    "        initial_count = df.count()\n",
    "        print(f\"Total inicial: {initial_count} registros\")\n",
    "\n",
    "        for col_name in columns:\n",
    "            Q1, Q3 = df.approxQuantile(col_name, [0.25, 0.75], quantile_error)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            if IQR == 0:\n",
    "                print(f\"Advertencia: IQR=0 para '{col_name}', se omite filtro de atípicos.\")\n",
    "                continue\n",
    "\n",
    "            before_count = df.count()\n",
    "            df = df.filter((F.col(col_name) >= lower_bound) & (F.col(col_name) <= upper_bound))\n",
    "            after_count = df.count()\n",
    "            removed = before_count - after_count\n",
    "\n",
    "            print(f\"'{col_name}': límites [{lower_bound:.2f}, {upper_bound:.2f}], eliminados {removed} registros ({removed / before_count:.2%})\")\n",
    "        \n",
    "        final_count = df.count()\n",
    "        print(f\"Total final después de eliminar atípicos: {final_count} registros\")\n",
    "        print(f\"Total eliminado: {initial_count - final_count} registros ({(initial_count - final_count) / initial_count:.2%})\\n\")\n",
    "        \n",
    "        return df\n",
    "   \n",
    "\n",
    "    def encode_categoricals(self, df: DataFrame) -> DataFrame:\n",
    "        categorical_cols = [\"vine\", \"verified_purchase\"]\n",
    "        for col_name in categorical_cols:\n",
    "            index_col = f\"{col_name}_index\"\n",
    "            self.indexed_columns.append(index_col)\n",
    "            indexer = StringIndexer(inputCol=col_name, outputCol=index_col)\n",
    "            df = indexer.fit(df).transform(df)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6dc0fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RELEVANT_COLUMNS_FOR_MODELING = [\n",
    "    \"star_rating\", \"helpful_votes\", \"total_votes\",\n",
    "    \"vine\", \"verified_purchase\", \"sentiment\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51060b89",
   "metadata": {},
   "source": [
    "#### Unión de particiones para construir M\n",
    "\n",
    "En esta primera etapa, se lleva a cabo la unión de múltiples subconjuntos muestreados de la base original, generados a partir de combinaciones de las variables de caracterización (`star_rating`, `verified_purchase`, `vine`). Esto será de ayuda para contruir un conjunto representativo (M) que conserve la diversidad presente en la base original, evitando sesgos asociados a una partición única."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6c52729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear instancia del Preprocessor\n",
    "preprocessor = Preprocessor(columns_for_modeling=RELEVANT_COLUMNS_FOR_MODELING)\n",
    "\n",
    "# Combinar todas las particiones de entrenamiento (Tri) en un solo DataFrame\n",
    "df_train = preprocessor.merge_partitions(train_sets)\n",
    "\n",
    "# Combinar todas las particiones de prueba (Tsi) en un solo DataFrame\n",
    "df_test = preprocessor.merge_partitions(test_sets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef8c94",
   "metadata": {},
   "source": [
    "#### Limpieza de valores nulos\n",
    "\n",
    "Para poder entrenar los modelos, es importante realizar una limpieza de valores faltantes para evitar errores en el prepocesamiento y mejorar la calidad de estos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "440b88f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN SET ===\n",
      "Total de registros antes de eliminar nulos: 581159\n",
      "Total de registros después de eliminar nulos: 581159\n",
      "Se eliminaron 0 registros (0.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TRAIN SET ===\")\n",
    "df_train = preprocessor.drop_nulls(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26e7c642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST SET ===\n",
      "Total de registros antes de eliminar nulos: 248658\n",
      "Total de registros después de eliminar nulos: 248658\n",
      "Se eliminaron 0 registros (0.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TEST SET ===\")\n",
    "df_test = preprocessor.drop_nulls(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc5275",
   "metadata": {},
   "source": [
    "#### Conversión de tipos\n",
    "\n",
    "En este paso, nos aseguraremos que las variables posean los tipos de datos correctos para que puedan ser procesados correctamente por los algoritmos de ML.\n",
    "\n",
    "Al utilizar `printSchema()`, se puede apreciar que las variables numéricas (`star_rating`, `helpful_votes`, `total_votes` y `sentiment`) y categóricas (`verified_purchase` y `vine`) están correctamente tipadas.\n",
    "\n",
    "Esta validación es importante, para evitar errores durante el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33218ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN SET ===\n",
      "root\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_parent: integer (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- vine: string (nullable = true)\n",
      " |-- verified_purchase: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: date (nullable = true)\n",
      " |-- sentiment: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TRAIN SET ===\")\n",
    "df_train.printSchema()\n",
    "# df_train = preprocessor.cast_types(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f32f4854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST SET ===\n",
      "root\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_parent: integer (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- vine: string (nullable = true)\n",
      " |-- verified_purchase: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: date (nullable = true)\n",
      " |-- sentiment: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TEST SET ===\")\n",
    "df_test.printSchema()\n",
    "# df_test = preprocessor.cast_types(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485566e3",
   "metadata": {},
   "source": [
    "#### Detección y tratamiento de valores atípicos\n",
    "\n",
    "Para evitar que los valores extremos puedan distorsionar las distribuciones e impactar sobre la capacidad de generalización de los modelos, se aplicó una técnica de detección y eliminación de valores atípicos (outliers) sobre las columnas `total_votes` y `helpful_votes`, utilizando el método del rango intercuartílico (IQR). Se decidió utilizar IQR, ya que es una técnica robusta que identifica outliers con base en la dispersión natural de los datos, en lugar de utilizar un umbral o threshold manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b12d093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN SET ===\n",
      "Total inicial: 581159 registros\n",
      "'helpful_votes': límites [-1.50, 2.50], eliminados 51992 registros (8.95%)\n",
      "'total_votes': límites [-1.50, 2.50], eliminados 22181 registros (4.19%)\n",
      "Total final después de eliminar atípicos: 506986 registros\n",
      "Total eliminado: 74173 registros (12.76%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TRAIN SET ===\")\n",
    "df_train = preprocessor.remove_outliers(df_train, columns=[\"helpful_votes\", \"total_votes\"], quantile_error=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7061eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST SET ===\n",
      "Total inicial: 248658 registros\n",
      "'helpful_votes': límites [-1.50, 2.50], eliminados 22084 registros (8.88%)\n",
      "'total_votes': límites [-1.50, 2.50], eliminados 9444 registros (4.17%)\n",
      "Total final después de eliminar atípicos: 217130 registros\n",
      "Total eliminado: 31528 registros (12.68%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== TEST SET ===\")\n",
    "df_test = preprocessor.remove_outliers(df_test, columns=[\"helpful_votes\", \"total_votes\"], quantile_error=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac70790",
   "metadata": {},
   "source": [
    "#### Transformación de variables categóricas\n",
    "\n",
    "Como se revisó anteriormente, se cuenta con variables categóricas (`verified_purchase` y `vine`), por lo que es relevante llevar a cabo una codificación para el entrenamiento de los modelos. Se decidió utilizar `StringIndexer` que es una técnica de codificación por frecuencia de aparación que no incrementa el número de columnas y funciona bien para categorías binarias. En este caso, ambas columnas poseen valores de 'Y' y 'N'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66d65e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|vine_index|verified_purchase_index|\n",
      "+----------+-----------------------+\n",
      "|1.0       |1.0                    |\n",
      "|0.0       |1.0                    |\n",
      "|0.0       |0.0                    |\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = preprocessor.encode_categoricals(df_train)\n",
    "df_train.select(\"vine_index\", \"verified_purchase_index\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "697501fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|vine_index|verified_purchase_index|\n",
      "+----------+-----------------------+\n",
      "|1.0       |1.0                    |\n",
      "|0.0       |1.0                    |\n",
      "|0.0       |0.0                    |\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = preprocessor.encode_categoricals(df_test)\n",
    "df_test.select(\"vine_index\", \"verified_purchase_index\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ff7ad6",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "Adicionalmenrte, se decidió crear nuevas variables para experimentar sobre el desempeño del modelo. Se espera que con las siguientes columnas, el modelo sea capaz de capturar mejor los patrones y relaciones:\n",
    "- `vote_ratio`: Representa la proporción de votos útiles entre todos los votos recibidos en una reseña.\n",
    "- `log_total_votes` y `log_total_votes`: Al aplicar transformación logarítmica, se reduce el impacto de los valores extremos y permite que el modelo aprenda mejor sobre una escala homogénea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7dd6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train \\\n",
    "    .withColumn(\"vote_ratio\", col(\"helpful_votes\") / (col(\"total_votes\") + 1)) \\\n",
    "    .withColumn(\"log_total_votes\", log1p(col(\"total_votes\"))) \\\n",
    "    .withColumn(\"log_helpful_votes\", log1p(col(\"helpful_votes\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cad3f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test \\\n",
    "    .withColumn(\"vote_ratio\", col(\"helpful_votes\") / (col(\"total_votes\") + 1)) \\\n",
    "    .withColumn(\"log_total_votes\", log1p(col(\"total_votes\"))) \\\n",
    "    .withColumn(\"log_helpful_votes\", log1p(col(\"helpful_votes\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5625be",
   "metadata": {},
   "source": [
    "#### Selección y entrenamiento de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3b0c58",
   "metadata": {},
   "source": [
    "En esta sección se entrenan dos modelos supervisados para predecir el sentimiento de las reseñas: **Random Forest** y **Gradient Boosted Trees (GBT)**. \n",
    "\n",
    "**Random Forest**\n",
    "* Se usó `VectorAssembler` para combinar las variables seleccionadas (`log_helpful_votes`, `log_total_votes`, `vote_ratio`, `vine_index`, `verified_purchase_index`) en un único vector de entrada, seguido por un `RandomForestClassifier`.\n",
    "* Se empleó un `CrossValidator` para explorar combinaciones de número de árboles (`numTrees`: 20, 50) y profundidad máxima (`maxDepth`: 5, 10), usando 3 pliegues de validación cruzada.\n",
    "\n",
    "**GBT**\n",
    "* Al igual que en el Random Forest, se utilizó un pipeline con `VectorAssembler` seguido por un `GBTClassifier`.\n",
    "* Se exploraron combinaciones de número máximo de iteraciones (`maxIter`: 20, 50), profundidad máxima (`maxDepth`: 3, 5) y tamaño del paso (`stepSize`: 0.05, 0.1), usando validación cruzada.\n",
    "\n",
    "Los modelos son evaluados usando las métricas seleccionadas (accuracy, precision, recall, F1, AUC-ROC) en la sección **3. Selección de métricas para medir calidad de resultados**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0baa320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_forest_cv(df_train, df_test, feature_cols, seed=42):\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"sentiment\", seed=seed)\n",
    "    # rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"sentiment\", seed=seed, weightCol=\"class_weight\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, [20, 50]) \\\n",
    "        .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "        .build()\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"sentiment\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "    cv = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3, seed=seed)\n",
    "\n",
    "    cv_model = cv.fit(df_train)\n",
    "    predictions = cv_model.transform(df_test)\n",
    "\n",
    "    accuracy = evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "    f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
    "    precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
    "    recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
    "\n",
    "    print(f\"Random Forest (CV) Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Random Forest (CV) F1 Score: {f1:.4f}\")\n",
    "    print(f\"Random Forest (CV) Precision: {precision:.4f}\")\n",
    "    print(f\"Random Forest (CV) Recall: {recall:.4f}\")\n",
    "\n",
    "    if df_train.select('sentiment').distinct().count() == 2:\n",
    "        binary_evaluator = BinaryClassificationEvaluator(labelCol=\"sentiment\", rawPredictionCol=\"rawPrediction\")\n",
    "        auc = binary_evaluator.evaluate(predictions)\n",
    "        print(f\"Random Forest (CV) AUC-ROC: {auc:.4f}\")\n",
    "    \n",
    "    return cv_model, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ccbc7af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"log_helpful_votes\", \n",
    "    \"log_total_votes\", \n",
    "    \"vote_ratio\", \n",
    "    \"vine_index\", \n",
    "    \"verified_purchase_index\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6429c21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (CV) Accuracy: 0.7959\n",
      "Random Forest (CV) F1 Score: 0.7152\n",
      "Random Forest (CV) Precision: 0.7483\n",
      "Random Forest (CV) Recall: 0.7959\n",
      "Random Forest (CV) AUC-ROC: 0.5810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Ejecutar el modelo Random Forest con validación cruzada\n",
    "rf_model, rf_predictions = run_random_forest_cv(\n",
    "    df_train,\n",
    "    df_test,\n",
    "    feature_cols=feature_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7018d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gbt_cv(df_train, df_test, feature_cols, seed=42):\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"sentiment\", seed=seed)\n",
    "\n",
    "    pipeline = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "    param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(gbt.maxIter, [20, 50]) \\\n",
    "        .addGrid(gbt.maxDepth, [3, 5]) \\\n",
    "        .addGrid(gbt.stepSize, [0.05, 0.1]) \\\n",
    "        .build()\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"sentiment\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "    cv = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3, seed=seed)\n",
    "\n",
    "    cv_model = cv.fit(df_train)\n",
    "    predictions = cv_model.transform(df_test)\n",
    "\n",
    "    accuracy = evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "    f1 = evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
    "    precision = evaluator.setMetricName(\"weightedPrecision\").evaluate(predictions)\n",
    "    recall = evaluator.setMetricName(\"weightedRecall\").evaluate(predictions)\n",
    "\n",
    "    print(f\"GBT (CV) Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"GBT (CV) F1 Score: {f1:.4f}\")\n",
    "    print(f\"GBT (CV) Precision: {precision:.4f}\")\n",
    "    print(f\"GBT (CV) Recall: {recall:.4f}\")\n",
    "\n",
    "    if df_train.select('sentiment').distinct().count() == 2:\n",
    "        binary_evaluator = BinaryClassificationEvaluator(labelCol=\"sentiment\", rawPredictionCol=\"rawPrediction\")\n",
    "        auc = binary_evaluator.evaluate(predictions)\n",
    "        print(f\"GBT (CV) AUC-ROC: {auc:.4f}\")\n",
    "    \n",
    "    return cv_model, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21d9f4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT (CV) Accuracy: 0.7959\n",
      "GBT (CV) F1 Score: 0.7152\n",
      "GBT (CV) Precision: 0.7483\n",
      "GBT (CV) Recall: 0.7959\n",
      "GBT (CV) AUC-ROC: 0.5926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Ejecutar el modelo GBT con validación cruzada\n",
    "gbt_model, gbt_predictions = run_gbt_cv(\n",
    "    df_train,\n",
    "    df_test,\n",
    "    feature_cols=feature_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26b2a12",
   "metadata": {},
   "source": [
    "## 5. Análisis de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c38b3",
   "metadata": {},
   "source": [
    "| **Métrica** | **Random Forest** | **GBT** |\n",
    "| ----------- | ----------------- | ------- |\n",
    "| Accuracy    | 0.7959            | 0.7959  |\n",
    "| F1 Score    | 0.7152            | 0.7152  |\n",
    "| Precision   | 0.7483            | 0.7483  |\n",
    "| Recall      | 0.7959            | 0.7959  |\n",
    "| AUC-ROC     | 0.5810            | 0.5926  |\n",
    "\n",
    "Después de entrenar los modelos Random Forest y Gradient Boosted Trees, ambos mostraron resultados muy similares, como se ve en la tabla anterior. Esto podría deberse a que el conjunto de datos presenta, posiblemente, patrones bastante claros, lo que facilita la tarea de clasificación. Ambos modelos lograron predecir correctamente cerca del 80% de las reseñas, con un equilibrio aceptable entre precisión y recall. No obstante, las métricas AUC-ROC (entre 0.58 y 0.59) fueron relativamente bajas, lo que sugiere que, si bien los modelos aciertan bien en una clase, tienen dificultades para diferenciar entre las clases.\n",
    "\n",
    "Como se exploró en la actividad 3, se ha identificado que existe un desbalance entre clases, con una fuerte predominancia de la clase positiva. Este desbalance puede limitar el desempeño de los modelos, particularmente en métricas sensibles como el AUC-ROC, y puede llevar a que los algoritmos favorezcan la clase mayoritaria para maximizar accuracy, dejando de lado una buena discriminación.\n",
    "\n",
    "**Reflexión Final**\n",
    "\n",
    "Personalmente, considero que esta actividad me ayudó a reconocer la importancia de trabajar con datos representativos y el proceso que implica llegar a una muestra que sea útil para analizar y entrenar nuestros modelos. También es relevante recordar que evaluar modelos de ML no se trata solo de elegir una métrica al azar, sino de entender qué mide cada una y cómo se alinea con los objetivos del problema o fenómeno estudiado. Métricas como el F1-score y el AUC-ROC ofrecen una perspectiva más completa sobre la capacidad del modelo para generalizar y discriminar entre clases. Finalmente, me gustaría destacar que el procesamiento paralelo, la persistencia en memoria y el guardado incremental son estrategias fundamentales para trabajar de forma eficiente y robusta con grandes volúmenes de datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
