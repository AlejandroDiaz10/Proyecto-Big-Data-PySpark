{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b1467a1",
   "metadata": {},
   "source": [
    "# Actividad 3 | Aprendizaje supervisado y no supervisado\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7f2b9",
   "metadata": {},
   "source": [
    "**MAESTRÍA EN INTELIGENCIA ARTIFICIAL APLICADA**\n",
    "\n",
    "**Curso: TC4034.10 - Análisis de grandes volúmenes de datos**\n",
    "\n",
    "Tecnológico de Monterrey\n",
    "\n",
    "* Dr. Iván Olmos Pineda\n",
    "* Mtra. Verónica Sandra Guzmán de Valle\n",
    "* Mtro. Alberto Daniel Salinas Montemayor\n",
    "\n",
    "**Actividad 3** - \n",
    "Aprendizaje supervisado y no supervisado\n",
    "\n",
    "**Fecha de entrega** - \n",
    "25 de mayo del 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc25f4",
   "metadata": {},
   "source": [
    "**Presenta**\n",
    "\n",
    "|  NOMBRE COMPLETO                        |     MATRÍCULA     |\n",
    "| :-------------------------------------: |:-----------------:|\n",
    "| Alejandro Díaz Villagómez               |  A01276769        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f3a621",
   "metadata": {},
   "source": [
    "# 1) Introducción teórica\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c764101",
   "metadata": {},
   "source": [
    "## **Aprendizaje supervisado y no supervisado en PySpark**\n",
    "\n",
    "El análisis y modelado de datos en entornos de Big Data requiere enfoques escalables que permitan procesar grandes volúmenes de información de forma eficiente. En este contexto, el aprendizaje automático (machine learning) ofrece un conjunto de técnicas y algoritmos que permiten extraer patrones útiles a partir de los datos. Estos algoritmos se agrupan tradicionalmente en dos grandes paradigmas: aprendizaje supervisado y aprendizaje no supervisado.\n",
    "\n",
    "### **Aprendizaje supervisado**\n",
    "\n",
    "El aprendizaje supervisado se basa en el uso de un conjunto de datos etiquetado, donde cada instancia posee una variable objetivo conocida (etiqueta o clase), que se desea predecir. El algoritmo aprende una función de mapeo a partir de ejemplos, con el objetivo de predecir la salida correspondiente para nuevas entradas. Esta técnica es ampliamente utilizada en problemas de clasificación y regresión.\n",
    "\n",
    "Entre los algoritmos más representativos del aprendizaje supervisado se encuentran:\n",
    "\n",
    "* Árboles de decisión (Decision Trees)\n",
    "* Bosques aleatorios (Random Forests)\n",
    "* Gradiente boosting (GBTClassifier)\n",
    "* Redes neuronales multicapa (Multilayer Perceptron Classifier)\n",
    "\n",
    "Estos algoritmos están implementados directamente en la librería pyspark.ml.classification, lo que permite su aplicación sobre grandes volúmenes de datos distribuidos con eficiencia.\n",
    "\n",
    "### **Aprendizaje no supervisado**\n",
    "\n",
    "En contraste, el aprendizaje no supervisado trabaja sobre datos sin etiquetas predefinidas, con el objetivo de encontrar estructuras ocultas o agrupaciones dentro de los datos. Es particularmente útil en tareas de segmentación de clientes, análisis exploratorio y reducción de dimensionalidad.\n",
    "\n",
    "Entre los algoritmos más reconocidos se encuentran:\n",
    "\n",
    "* K-Means Clustering\n",
    "* Gaussian Mixture Models (GMM)\n",
    "* Power Iteration Clustering (PIC)\n",
    "\n",
    "Estos modelos también se encuentran disponibles en PySpark, a través del módulo pyspark.ml.clustering, el cual permite realizar agrupamientos escalables con configuraciones flexibles sobre datasets distribuidos.\n",
    "\n",
    "### **Disponibilidad en PySpark**\n",
    "\n",
    "PySpark, como API de Apache Spark para Python, ofrece una amplia gama de algoritmos de ambos enfoques dentro del paquete pyspark.ml. Esto permite diseñar flujos completos de aprendizaje automático que incluyan:\n",
    "\n",
    "* Preparación de datos (transformaciones, codificación, escalado)\n",
    "* División en entrenamiento y prueba\n",
    "* Entrenamiento del modelo\n",
    "* Evaluación de métricas de desempeño\n",
    "* Predicción de nuevas instancias\n",
    "\n",
    "Gracias a esta integración, PySpark se consolida como una herramienta clave para el desarrollo de modelos de machine learning a gran escala, tanto supervisados como no supervisados.\n",
    "\n",
    "### **Consideraciones de rendimiento**\n",
    "\n",
    "Una de las principales ventajas de PySpark frente a bibliotecas tradicionales de aprendizaje automático (como scikit-learn) radica en su capacidad de procesamiento distribuido. Al ejecutarse sobre clústeres de Spark, PySpark permite manejar datasets de gran volumen sin necesidad de cargarlos completamente en memoria, lo que lo convierte en una solución óptima para entornos empresariales o científicos con requerimientos de escalabilidad y eficiencia computacional.\n",
    "\n",
    "### **Referencias**\n",
    "\n",
    "- [Supervised versus unsupervised learning: What's the difference?](https://www.ibm.com/think/topics/supervised-vs-unsupervised-learning)\n",
    "- [Supervised and Unsupervised learning](https://www.geeksforgeeks.org/supervised-unsupervised-learning/)\n",
    "- [Building Machine Learning Models with PySpark's pyspark.ml Library: A Comprehensive Guide.](https://dev.to/grayhat/building-machine-learning-models-with-pysparks-pysparkml-library-a-comprehensive-guide-4g5h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6428743",
   "metadata": {},
   "source": [
    "# 2) Selección de los datos\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c8a53a",
   "metadata": {},
   "source": [
    "## 2.1) Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80669f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install findspark pyspark setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0005c201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alejandrodiazvillagomez/Desktop/Proyecto-Big-Data-PySpark/.venv/lib/python3.12/site-packages/pyspark'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from itertools import product\n",
    "from os import path\n",
    "\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71a57040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/20 20:48:10 WARN Utils: Your hostname, Alejandros-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.100.220 instead (on interface en0)\n",
      "25/05/20 20:48:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/20 20:48:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.220:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x107df34a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f9331cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileManager():\n",
    "    @staticmethod\n",
    "    def open_csv_file(file_path : str):\n",
    "        \"\"\"\n",
    "        This method opens a csv file with pyspark\n",
    "        \"\"\"\n",
    "        csv_df = spark.read.csv(\n",
    "            file_path,\n",
    "            header=True,\n",
    "            inferSchema=True,\n",
    "            multiLine=True,\n",
    "            escape=\"\\\"\",\n",
    "            quote=\"\\\"\"\n",
    "        )\n",
    "\n",
    "        # csv_df.show(truncate=20)\n",
    "\n",
    "        return csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9183760c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandrodiazvillagomez/Desktop/Proyecto-Big-Data-PySpark/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/alejandrodiazvillagomez/.cache/kagglehub/datasets/machharavikiran/amazon-reviews/versions/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>22873041</td>\n",
       "      <td>R3ARRMDEGED8RD</td>\n",
       "      <td>B00KJWQIIC</td>\n",
       "      <td>335625766</td>\n",
       "      <td>Plemo 14-Inch Laptop Sleeve Case Waterproof Fa...</td>\n",
       "      <td>PC</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Pleasantly surprised</td>\n",
       "      <td>I was very surprised at the high quality of th...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>30088427</td>\n",
       "      <td>RQ28TSA020Y6J</td>\n",
       "      <td>B013ALA9LA</td>\n",
       "      <td>671157305</td>\n",
       "      <td>TP-Link OnHub AC1900 Wireless Wi-Fi Router</td>\n",
       "      <td>PC</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>OnHub is a pretty no nonsense type router that...</td>\n",
       "      <td>I am a Google employee and had to chance to us...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>20329786</td>\n",
       "      <td>RUXJRZCT6953M</td>\n",
       "      <td>B00PML2GQ8</td>\n",
       "      <td>982036237</td>\n",
       "      <td>AmazonBasics USB 3.0 A Male to A Male Cable - ...</td>\n",
       "      <td>PC</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>None of them worked.  No functionality at all.</td>\n",
       "      <td>Bought cables in 3ft, 6ft and 9ft.  NONE of th...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>14215710</td>\n",
       "      <td>R7EO0UO6BPB71</td>\n",
       "      <td>B001NS0OZ4</td>\n",
       "      <td>576587596</td>\n",
       "      <td>Transcend P8 15-in-1 USB 2.0 Flash Memory Card...</td>\n",
       "      <td>PC</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>just keep searching.</td>\n",
       "      <td>nope, cheap and slow</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>38264512</td>\n",
       "      <td>R39NJY2YJ1JFSV</td>\n",
       "      <td>B00AQMTND2</td>\n",
       "      <td>964759214</td>\n",
       "      <td>Aleratec SATA Data Cable 2.0 20in Serial ATA S...</td>\n",
       "      <td>PC</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Excellent! Great value and does the job.</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>US</td>\n",
       "      <td>30548466</td>\n",
       "      <td>R31SR7REWNX7CF</td>\n",
       "      <td>B00KX4TORI</td>\n",
       "      <td>170101802</td>\n",
       "      <td>Kingston Digital MobileLite G4 USB 3.0 Multi-F...</td>\n",
       "      <td>PC</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Good quality, works well and compact</td>\n",
       "      <td>Good quality,works well and compact size</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>US</td>\n",
       "      <td>589298</td>\n",
       "      <td>RVBP8I1R0CTZ8</td>\n",
       "      <td>B00P17WEMY</td>\n",
       "      <td>206124740</td>\n",
       "      <td>White 9 Inch Unlocked Dual Sim Card Phone Tabl...</td>\n",
       "      <td>PC</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>in fact this is third China good. Demn s***</td>\n",
       "      <td>This demn tablet is just a Real Chinese produc...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>US</td>\n",
       "      <td>49329488</td>\n",
       "      <td>R1QF6RS1PDLU18</td>\n",
       "      <td>B00TR05L9Y</td>\n",
       "      <td>778403103</td>\n",
       "      <td>Lenovo TAB2 A10 - 10.1\" 2-in-1 Tablet (1.5Ghz,...</td>\n",
       "      <td>PC</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Good</td>\n",
       "      <td>I am not sure I don't know if it is the tablet...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>US</td>\n",
       "      <td>50728290</td>\n",
       "      <td>R23AICGEDAJQL1</td>\n",
       "      <td>B0098Y77OG</td>\n",
       "      <td>177098042</td>\n",
       "      <td>Acer</td>\n",
       "      <td>PC</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>You get what you pay for</td>\n",
       "      <td>After exactly 45 days, the screen went dark. P...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>US</td>\n",
       "      <td>37802374</td>\n",
       "      <td>R2EY3N4K9W19UP</td>\n",
       "      <td>B00IFYEYXC</td>\n",
       "      <td>602496520</td>\n",
       "      <td>AzureWave Broadcom BCM94352HMB 802.11/ac/867Mb...</td>\n",
       "      <td>PC</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Great for Windows 7 Laptop!</td>\n",
       "      <td>Replaced my Intel Centrino 2230 with the BCM94...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     22873041  R3ARRMDEGED8RD  B00KJWQIIC       335625766   \n",
       "1          US     30088427   RQ28TSA020Y6J  B013ALA9LA       671157305   \n",
       "2          US     20329786   RUXJRZCT6953M  B00PML2GQ8       982036237   \n",
       "3          US     14215710   R7EO0UO6BPB71  B001NS0OZ4       576587596   \n",
       "4          US     38264512  R39NJY2YJ1JFSV  B00AQMTND2       964759214   \n",
       "5          US     30548466  R31SR7REWNX7CF  B00KX4TORI       170101802   \n",
       "6          US       589298   RVBP8I1R0CTZ8  B00P17WEMY       206124740   \n",
       "7          US     49329488  R1QF6RS1PDLU18  B00TR05L9Y       778403103   \n",
       "8          US     50728290  R23AICGEDAJQL1  B0098Y77OG       177098042   \n",
       "9          US     37802374  R2EY3N4K9W19UP  B00IFYEYXC       602496520   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0  Plemo 14-Inch Laptop Sleeve Case Waterproof Fa...               PC   \n",
       "1         TP-Link OnHub AC1900 Wireless Wi-Fi Router               PC   \n",
       "2  AmazonBasics USB 3.0 A Male to A Male Cable - ...               PC   \n",
       "3  Transcend P8 15-in-1 USB 2.0 Flash Memory Card...               PC   \n",
       "4  Aleratec SATA Data Cable 2.0 20in Serial ATA S...               PC   \n",
       "5  Kingston Digital MobileLite G4 USB 3.0 Multi-F...               PC   \n",
       "6  White 9 Inch Unlocked Dual Sim Card Phone Tabl...               PC   \n",
       "7  Lenovo TAB2 A10 - 10.1\" 2-in-1 Tablet (1.5Ghz,...               PC   \n",
       "8                                               Acer               PC   \n",
       "9  AzureWave Broadcom BCM94352HMB 802.11/ac/867Mb...               PC   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0            5              0            0    N                 Y   \n",
       "1            5             24           31    N                 N   \n",
       "2            1              2            2    N                 N   \n",
       "3            1              0            0    N                 Y   \n",
       "4            5              0            0    N                 Y   \n",
       "5            5              0            0    N                 Y   \n",
       "6            3              1            2    N                 Y   \n",
       "7            4              1            1    N                 Y   \n",
       "8            1              0            0    N                 Y   \n",
       "9            5              3            4    N                 Y   \n",
       "\n",
       "                                     review_headline  \\\n",
       "0                               Pleasantly surprised   \n",
       "1  OnHub is a pretty no nonsense type router that...   \n",
       "2     None of them worked.  No functionality at all.   \n",
       "3                               just keep searching.   \n",
       "4                                         Five Stars   \n",
       "5               Good quality, works well and compact   \n",
       "6        in fact this is third China good. Demn s***   \n",
       "7                                               Good   \n",
       "8                           You get what you pay for   \n",
       "9                        Great for Windows 7 Laptop!   \n",
       "\n",
       "                                         review_body review_date  sentiment  \n",
       "0  I was very surprised at the high quality of th...  2015-08-31          1  \n",
       "1  I am a Google employee and had to chance to us...  2015-08-31          1  \n",
       "2  Bought cables in 3ft, 6ft and 9ft.  NONE of th...  2015-08-31          0  \n",
       "3                               nope, cheap and slow  2015-08-31          0  \n",
       "4           Excellent! Great value and does the job.  2015-08-31          1  \n",
       "5           Good quality,works well and compact size  2015-08-31          1  \n",
       "6  This demn tablet is just a Real Chinese produc...  2015-08-31          0  \n",
       "7  I am not sure I don't know if it is the tablet...  2015-08-31          1  \n",
       "8  After exactly 45 days, the screen went dark. P...  2015-08-31          0  \n",
       "9  Replaced my Intel Centrino 2230 with the BCM94...  2015-08-31          1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "FILE_PATH = kagglehub.dataset_download(\"machharavikiran/amazon-reviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", FILE_PATH)\n",
    "\n",
    "df_reviews = FileManager.open_csv_file(FILE_PATH)\n",
    "df_reviews.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52044b36",
   "metadata": {},
   "source": [
    "En esta primera etapa, se realiza la carga del conjunto de datos correspondiente a reseñas de productos de Amazon. \n",
    "\n",
    "El archivo fuente se obtiene directamente desde Kaggle utilizando la librería `kagglehub`, lo que garantiza la reproducibilidad del experimento mediante una fuente pública y estandarizada.\n",
    "\n",
    "Posteriormente, se emplea el método `open_csv_file()` de la clase `FileManager`, el cual encapsula la lógica de lectura del archivo utilizando PySpark. Esta función está configurada para:\n",
    "\n",
    "- Leer archivos en formato CSV con encabezados (`header=True`)\n",
    "- Inferir automáticamente los tipos de datos de cada columna (`inferSchema=True`)\n",
    "- Soportar estructuras multilinea en los campos textuales como `review_body` (`multiLine=True`)\n",
    "- Manejar correctamente comillas escapadas o anidadas (`escape=\"\\\"\"` y `quote=\"\\\"\"`)\n",
    "\n",
    "El resultado de este proceso es un `DataFrame` distribuido denominado `df_reviews`, que contiene un total de más de 6.9 millones de reseñas en idioma inglés correspondientes a la categoría de productos \"PC\".\n",
    "\n",
    "La correcta visualización de las primeras filas del dataset confirma que el archivo ha sido cargado de manera satisfactoria, y que los datos contienen información estructurada y adecuada para las siguientes etapas del proceso analítico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d25a26",
   "metadata": {},
   "source": [
    "## 2.2) Construcción de la muestra M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b290b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartitioningManager:\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_probabilities(df, cols):\n",
    "        \"\"\"\n",
    "        Computes and returns the probability of each combination of values in the specified columns.\n",
    "        \"\"\"\n",
    "        total_count = df.count()\n",
    "        return df.groupBy(cols).count() \\\n",
    "                 .withColumn(\"probability\", F.round(F.col(\"count\") / total_count, 6)) \\\n",
    "                 .orderBy(\"probability\", ascending=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_partition(df, star_rating, verified_purchase, vine):\n",
    "        \"\"\"\n",
    "        Filters the DataFrame by specific values for rating, verified purchase, and vine.\n",
    "        \"\"\"\n",
    "        return df.filter(\n",
    "            (F.col(\"star_rating\") == star_rating) &\n",
    "            (F.col(\"verified_purchase\") == verified_purchase) &\n",
    "            (F.col(\"vine\") == vine)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_all_partitions(df, min_probability=0.0001):\n",
    "        \"\"\"\n",
    "        Generates partitions only for combinations whose joint probability is above min_probability.\n",
    "        \"\"\"\n",
    "\n",
    "        prob_df = PartitioningManager.compute_probabilities(\n",
    "            df, [\"star_rating\", \"verified_purchase\", \"vine\"]\n",
    "        )\n",
    "\n",
    "        filtered_combinations = prob_df.filter(\n",
    "            F.col(\"probability\") >= min_probability\n",
    "        ).select(\"star_rating\", \"verified_purchase\", \"vine\").collect()\n",
    "\n",
    "        partitions = {}\n",
    "        for row in filtered_combinations:\n",
    "            rating = row[\"star_rating\"]\n",
    "            purchase = row[\"verified_purchase\"]\n",
    "            vine = row[\"vine\"]\n",
    "\n",
    "            key = f\"R{rating}_VP{purchase}_V{vine}\"\n",
    "            filtered = PartitioningManager.filter_partition(df, rating, purchase, vine)\n",
    "            # Add a partition key to the DataFrame\n",
    "            filtered = filtered.withColumn(\"partition_key\", F.lit(key))\n",
    "            partitions[key] = filtered\n",
    "            print(f\"Partition {key} created with {filtered.count()} records.\")\n",
    "\n",
    "        return partitions\n",
    "\n",
    "    @staticmethod\n",
    "    def stratified_sample_partitioned_data(partitions_dict, label_col=\"sentiment\", fraction=0.3, min_rows=50):\n",
    "        \"\"\"\n",
    "        Applies stratified sampling to each partition based on sentiment.\n",
    "        \"\"\"\n",
    "        sampled_partitions = {}\n",
    "\n",
    "        for key, df in partitions_dict.items():\n",
    "            count = df.count()\n",
    "\n",
    "            if count < min_rows:\n",
    "                print(f\"Skipping partition {key} — only {count} rows (<{min_rows})\")\n",
    "                continue\n",
    "\n",
    "            sentiments = df.select(label_col).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "            fractions = {s: fraction for s in sentiments}\n",
    "\n",
    "            sampled_df = df.sampleBy(label_col, fractions, seed=42)\n",
    "            sampled_partitions[key] = sampled_df\n",
    "            print(f\"Sampled {sampled_df.count()} rows from partition {key} (original: {count})\")\n",
    "\n",
    "        return sampled_partitions\n",
    "\n",
    "    @staticmethod\n",
    "    def consolidate_sampled_partitions(sampled_partitions):\n",
    "        \"\"\"\n",
    "        Combines all sampled partitions into a single DataFrame.\n",
    "        Prints validations about the final dataset.\n",
    "        \"\"\"\n",
    "        from functools import reduce\n",
    "        from pyspark.sql import DataFrame\n",
    "\n",
    "        if not sampled_partitions:\n",
    "            print(\"No partitions to consolidate.\")\n",
    "            return None\n",
    "\n",
    "        sampled_dfs = list(sampled_partitions.values())\n",
    "        df_M = reduce(DataFrame.unionByName, sampled_dfs)\n",
    "\n",
    "        print(\"Consolidation sampled partitions completed.\")\n",
    "        print(f\"Total number of records in the sample M: {df_M.count()}\")\n",
    "\n",
    "        print(\"Sentiment distribution in the sample M:\")\n",
    "        df_M.groupBy(\"sentiment\").count().orderBy(\"count\", ascending=False).show()\n",
    "        \n",
    "        print(\"Sampled partitions distribution:\")\n",
    "        df_M.groupBy(\"partition_key\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "        return df_M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b33329c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPY_VN created with 3679909 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R4_VPY_VN created with 1019728 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R1_VPY_VN created with 603371 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R3_VPY_VN created with 443364 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPN_VN created with 410073 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R2_VPY_VN created with 300544 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R1_VPN_VN created with 152779 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R4_VPN_VN created with 135197 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R3_VPN_VN created with 65398 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R2_VPN_VN created with 59973 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPN_VY created with 15604 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R4_VPN_VY created with 13240 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R3_VPN_VY created with 4886 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R2_VPN_VY created with 1634 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R1_VPN_VY created with 705 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition R5_VPY_VY created with 101 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "partitions = PartitioningManager.generate_all_partitions(df_reviews, min_probability=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473dd3d6",
   "metadata": {},
   "source": [
    "Una vez cargado el conjunto de datos completo, se procede a realizar el **particionamiento** de la base de datos `df_reviews` en subconjuntos homogéneos, basados en combinaciones de valores representativos de las variables de caracterización previamente identificadas.\n",
    "\n",
    "Para la generación de las particiones, se seleccionaron específicamente tres variables categóricas: `star_rating`, `verified_purchase` y `vine`. Esta elección responde a criterios de relevancia analítica, simplicidad estructural y viabilidad computacional:\n",
    "\n",
    "* **Relevancia analítica**: Estas variables están directamente asociadas a la percepción del cliente (`star_rating`), la credibilidad de la reseña (`verified_purchase`) y su posible sesgo promocional (`vine`), siendo por tanto altamente explicativas del comportamiento del consumidor.\n",
    "* **Evitar combinaciones poco frecuentes**: Incluir más variables de segmentación habría generado una *\"explosión\"* combinatoria en el número de particiones, muchas de las cuales serían extremadamente pequeñas o vacías, complicando tanto el análisis como el modelado.\n",
    "* **Viabilidad computacional**: Reducir la dimensionalidad del espacio de particionamiento ayuda a mantener el tamaño de muestra bajo control, garantizando que los procesos posteriores de muestreo y modelado sean eficientes y escalables.\n",
    "\n",
    "Este proceso se implementa a través del método `generate_all_partitions()` de la clase `PartitioningManager`. La lógica de este método se resume en los siguientes pasos:\n",
    "\n",
    "1. **Cálculo de probabilidades conjuntas**:\n",
    "   Se calcula la probabilidad de ocurrencia de todas las combinaciones posibles de las variables categóricas:\n",
    "   - `star_rating` (valoración otorgada)\n",
    "   - `verified_purchase` (indicador de compra verificada)\n",
    "   - `vine` (participación en el programa de reseñas Vine)\n",
    "\n",
    "   El objetivo es identificar combinaciones suficientemente frecuentes en la población como para constituir particiones relevantes.\n",
    "\n",
    "2. **Filtrado de combinaciones**:\n",
    "   Se utiliza un umbral de probabilidad mínima (`min_probability=0.00001`) para descartar combinaciones con ocurrencia insignificante, lo que permite evitar particiones demasiado pequeñas que no aporten valor analítico ni representatividad.\n",
    "\n",
    "3. **Construcción de particiones**:\n",
    "   Para cada combinación que supera el umbral de probabilidad, se crea una partición específica mediante filtrado del DataFrame original.\n",
    "   A cada subconjunto se le asigna un identificador único (`partition_key`) con el formato `R{rating}_VP{Y/N}_V{Y/N}`, que facilita su trazabilidad.\n",
    "\n",
    "   Por ejemplo:\n",
    "   - `R5_VPY_VN` representa todas las reseñas con 5 estrellas, compra verificada, y sin participación en el programa Vine.\n",
    "   - `R1_VPN_VY` representa reseñas con 1 estrella, sin compra verificada, pero con participación en Vine.\n",
    "\n",
    "4. **Resultado**:\n",
    "   El proceso genera un total de 16 particiones distintas con tamaños muy variados, lo cual es consistente con la distribución sesgada del dataset hacia reseñas positivas y verificadas.\n",
    "   La partición `R5_VPY_VN` es la más numerosa con más de 3.6 millones de registros, mientras que otras como `R5_VPY_VY` o `R1_VPN_VY` tienen tamaños mucho más reducidos, lo que refleja escenarios menos comunes dentro de la plataforma.\n",
    "\n",
    "Este paso es crucial, ya que permite segmentar la base de datos original en grupos de usuarios y comportamientos claramente definidos, lo que a su vez facilita:\n",
    "\n",
    "- La aplicación de estrategias de muestreo diferenciadas,\n",
    "- El análisis comparativo entre segmentos,\n",
    "- La generación de modelos más precisos y representativos en etapas posteriores.\n",
    "\n",
    "Además, garantiza que el análisis posterior no mezcle contextos heterogéneos, conservando la coherencia entre características de los usuarios y sus evaluaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d48b499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 1473392 rows from partition R5_VPY_VN (original: 3679909)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 408759 rows from partition R4_VPY_VN (original: 1019728)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 241961 rows from partition R1_VPY_VN (original: 603371)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 177507 rows from partition R3_VPY_VN (original: 443364)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 164237 rows from partition R5_VPN_VN (original: 410073)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 120365 rows from partition R2_VPY_VN (original: 300544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 61298 rows from partition R1_VPN_VN (original: 152779)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 54241 rows from partition R4_VPN_VN (original: 135197)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 26223 rows from partition R3_VPN_VN (original: 65398)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 24060 rows from partition R2_VPN_VN (original: 59973)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 6330 rows from partition R5_VPN_VY (original: 15604)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 5385 rows from partition R4_VPN_VY (original: 13240)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 1979 rows from partition R3_VPN_VY (original: 4886)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 682 rows from partition R2_VPN_VY (original: 1634)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 294 rows from partition R1_VPN_VY (original: 705)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 203:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 35 rows from partition R5_VPY_VY (original: 101)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sampled_partitions = PartitioningManager.stratified_sample_partitioned_data(partitions, fraction=0.4, min_rows=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39040df1",
   "metadata": {},
   "source": [
    "Con el objetivo de construir una muestra de trabajo contenida pero representativa, se procede a aplicar una técnica de **muestreo estratificado** dentro de cada una de las particiones generadas en el paso anterior.\n",
    "\n",
    "Este proceso se lleva a cabo utilizando el método `stratified_sample_partitioned_data()` de la clase `PartitioningManager`, el cual opera bajo los siguientes principios:\n",
    "\n",
    "1. **Muestreo por clase de salida (`sentiment`)**:\n",
    "   Cada partición contiene reseñas que han sido clasificadas como positivas (`sentiment = 1`) o negativas (`sentiment = 0`). \n",
    "   Para preservar esta distribución y evitar sesgos en el modelado posterior, se aplica un muestreo **estratificado** con respecto a dicha variable. Esto significa que se extrae la misma proporción de registros para cada clase dentro de cada partición.\n",
    "\n",
    "2. **Fracción de muestreo definida**:\n",
    "   En este caso, se especifica una fracción del `40%` (`fraction=0.4`) para cada clase. Esto implica que se extraerá el 40% de los registros positivos y el 40% de los negativos por partición, siempre que la cantidad total de registros sea suficiente.\n",
    "\n",
    "3. **Control de calidad mínimo por partición**:\n",
    "   Para garantizar la validez estadística de cada subconjunto extraído, se define un umbral mínimo de registros (`min_rows=100`). \n",
    "   Las particiones que no alcanzan esta cantidad son automáticamente descartadas del muestreo.\n",
    "\n",
    "4. **Resultados del muestreo**:\n",
    "   Como resultado del proceso, se logra extraer una submuestra significativa desde cada partición. Por ejemplo:\n",
    "   - De la partición más abundante `R5_VPY_VN` (3.6 millones de registros), se extrajeron más de 1.47 millones.\n",
    "   - De particiones medianas como `R1_VPY_VN` o `R3_VPN_VN`, se obtuvieron decenas de miles de observaciones.\n",
    "   - De particiones poco frecuentes como `R5_VPY_VY`, se extrajeron 35 registros, lo que refleja la escasez de ese patrón en la población.\n",
    "\n",
    "Este tipo de muestreo es altamente recomendable en problemas de clasificación con clases desbalanceadas, ya que:\n",
    "\n",
    "- Ayuda a conservar la proporción natural de clases dentro de cada segmento del dataset.\n",
    "- Mejora la estabilidad de los modelos al asegurar exposición balanceada a distintas opiniones.\n",
    "- Evita que las clases mayoritarias dominen el entrenamiento de algoritmos supervisados.\n",
    "\n",
    "Además de asegurar representatividad, este enfoque optimiza el rendimiento computacional y la calidad de los modelos de machine learning:\n",
    "\n",
    "- **Reducción del tamaño total**: Se pasa de ~7 millones de registros a una muestra de ~2.7 millones, mejorando tiempos de procesamiento y consumo de memoria.\n",
    "- **Eliminación de grupos marginales**: Se omiten combinaciones con baja ocurrencia, que suelen aportar ruido o aumentar la varianza de los modelos.\n",
    "\n",
    "La salida de esta etapa es un nuevo diccionario `sampled_partitions`, donde cada clave corresponde a una partición válida y cada valor es un subconjunto muestreado de esa partición, listo para ser consolidado en el paso siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aac99d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidation sampled partitions completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records in the sample M: 2766748\n",
      "Sentiment distribution in the sample M:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|sentiment|  count|\n",
      "+---------+-------+\n",
      "|        1|2112379|\n",
      "|        0| 654369|\n",
      "+---------+-------+\n",
      "\n",
      "Sampled partitions distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 212:=========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|partition_key|  count|\n",
      "+-------------+-------+\n",
      "|    R5_VPY_VN|1473392|\n",
      "|    R4_VPY_VN| 408759|\n",
      "|    R1_VPY_VN| 241961|\n",
      "|    R3_VPY_VN| 177507|\n",
      "|    R5_VPN_VN| 164237|\n",
      "|    R2_VPY_VN| 120365|\n",
      "|    R1_VPN_VN|  61298|\n",
      "|    R4_VPN_VN|  54241|\n",
      "|    R3_VPN_VN|  26223|\n",
      "|    R2_VPN_VN|  24060|\n",
      "|    R5_VPN_VY|   6330|\n",
      "|    R4_VPN_VY|   5385|\n",
      "|    R3_VPN_VY|   1979|\n",
      "|    R2_VPN_VY|    682|\n",
      "|    R1_VPN_VY|    294|\n",
      "|    R5_VPY_VY|     35|\n",
      "+-------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_M = PartitioningManager.consolidate_sampled_partitions(sampled_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "815e689f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>partition_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>37802374</td>\n",
       "      <td>R2EY3N4K9W19UP</td>\n",
       "      <td>B00IFYEYXC</td>\n",
       "      <td>602496520</td>\n",
       "      <td>AzureWave Broadcom BCM94352HMB 802.11/ac/867Mb...</td>\n",
       "      <td>PC</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Great for Windows 7 Laptop!</td>\n",
       "      <td>Replaced my Intel Centrino 2230 with the BCM94...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "      <td>R5_VPY_VN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>13232866</td>\n",
       "      <td>R21PTQNLGCBN0I</td>\n",
       "      <td>B00XMN20Y6</td>\n",
       "      <td>873354048</td>\n",
       "      <td>Fintie iPad 2/3/4 Case - Slim Fit Folio Case w...</td>\n",
       "      <td>PC</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Nice color, I love it</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "      <td>R5_VPY_VN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>34685412</td>\n",
       "      <td>R1LTMCKOL72U34</td>\n",
       "      <td>9875961809</td>\n",
       "      <td>982116513</td>\n",
       "      <td>Professional Ultra SanDisk 16GB MicroSDHC Card...</td>\n",
       "      <td>PC</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Works great to store my pics and videos!</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "      <td>R5_VPY_VN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>12680846</td>\n",
       "      <td>R2AG4YGHD0IB1C</td>\n",
       "      <td>B00CC47VAO</td>\n",
       "      <td>170539622</td>\n",
       "      <td>25-PACK CF Memory Card Plastic Storage Jewel C...</td>\n",
       "      <td>PC</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great protector case just sqeeze the sides and...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "      <td>R5_VPY_VN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>6348379</td>\n",
       "      <td>R1B0GR7BIW6ZFM</td>\n",
       "      <td>B00SB4AHJG</td>\n",
       "      <td>868501171</td>\n",
       "      <td>Lenovo Z70 (80FG00DBUS) 17.3\" Laptop</td>\n",
       "      <td>PC</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Great</td>\n",
       "      <td>I love this computer. The big screen, the spee...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "      <td>R5_VPY_VN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>US</td>\n",
       "      <td>2701611</td>\n",
       "      <td>R3SQFM4D091HWE</td>\n",
       "      <td>B004EBG4J2</td>\n",
       "      <td>582958308</td>\n",
       "      <td>Eastvita MK-200 Keyboard and Case for 7-Inch T...</td>\n",
       "      <td>PC</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>get this</td>\n",
       "      <td>case is well made and keyboard comes in handy.</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "      <td>R5_VPY_VN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>US</td>\n",
       "      <td>47012791</td>\n",
       "      <td>R3ILJ0GC2105AV</td>\n",
       "      <td>B00KT1HL5M</td>\n",
       "      <td>979894134</td>\n",
       "      <td>Fintie Samsung Galaxy Tab 4 10.1 SmartShell</td>\n",
       "      <td>PC</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Love</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "      <td>R5_VPY_VN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>US</td>\n",
       "      <td>18382586</td>\n",
       "      <td>RM7WHMA7RL279</td>\n",
       "      <td>B00B0N29G4</td>\n",
       "      <td>243601768</td>\n",
       "      <td>MoKo Slim Fit Cover Case for iPad Air Parent.</td>\n",
       "      <td>PC</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Just what we wanted!</td>\n",
       "      <td>Fits on my Dad's iPad well. He loves it. It fe...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "      <td>R5_VPY_VN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>US</td>\n",
       "      <td>20961846</td>\n",
       "      <td>R1F854X0L51448</td>\n",
       "      <td>B00ZWUEN10</td>\n",
       "      <td>684076400</td>\n",
       "      <td>Botetrade Children School Backpack Bags for Pr...</td>\n",
       "      <td>PC</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Good deal</td>\n",
       "      <td>Very happy and totally satisfied with this pur...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "      <td>R5_VPY_VN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>US</td>\n",
       "      <td>44080268</td>\n",
       "      <td>R2RUAQ0M6MA30T</td>\n",
       "      <td>B005WUUFBW</td>\n",
       "      <td>708772551</td>\n",
       "      <td>ASUS M5A78L-M LX PLUS AM3+ AMD 760G Micro ATX ...</td>\n",
       "      <td>PC</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Exactly what i needed. Thanks!</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "      <td>R5_VPY_VN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     37802374  R2EY3N4K9W19UP  B00IFYEYXC       602496520   \n",
       "1          US     13232866  R21PTQNLGCBN0I  B00XMN20Y6       873354048   \n",
       "2          US     34685412  R1LTMCKOL72U34  9875961809       982116513   \n",
       "3          US     12680846  R2AG4YGHD0IB1C  B00CC47VAO       170539622   \n",
       "4          US      6348379  R1B0GR7BIW6ZFM  B00SB4AHJG       868501171   \n",
       "5          US      2701611  R3SQFM4D091HWE  B004EBG4J2       582958308   \n",
       "6          US     47012791  R3ILJ0GC2105AV  B00KT1HL5M       979894134   \n",
       "7          US     18382586   RM7WHMA7RL279  B00B0N29G4       243601768   \n",
       "8          US     20961846  R1F854X0L51448  B00ZWUEN10       684076400   \n",
       "9          US     44080268  R2RUAQ0M6MA30T  B005WUUFBW       708772551   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0  AzureWave Broadcom BCM94352HMB 802.11/ac/867Mb...               PC   \n",
       "1  Fintie iPad 2/3/4 Case - Slim Fit Folio Case w...               PC   \n",
       "2  Professional Ultra SanDisk 16GB MicroSDHC Card...               PC   \n",
       "3  25-PACK CF Memory Card Plastic Storage Jewel C...               PC   \n",
       "4               Lenovo Z70 (80FG00DBUS) 17.3\" Laptop               PC   \n",
       "5  Eastvita MK-200 Keyboard and Case for 7-Inch T...               PC   \n",
       "6        Fintie Samsung Galaxy Tab 4 10.1 SmartShell               PC   \n",
       "7      MoKo Slim Fit Cover Case for iPad Air Parent.               PC   \n",
       "8  Botetrade Children School Backpack Bags for Pr...               PC   \n",
       "9  ASUS M5A78L-M LX PLUS AM3+ AMD 760G Micro ATX ...               PC   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0            5              3            4    N                 Y   \n",
       "1            5              0            0    N                 Y   \n",
       "2            5              0            0    N                 Y   \n",
       "3            5              0            0    N                 Y   \n",
       "4            5              0            1    N                 Y   \n",
       "5            5              0            0    N                 Y   \n",
       "6            5              0            0    N                 Y   \n",
       "7            5              0            0    N                 Y   \n",
       "8            5              0            0    N                 Y   \n",
       "9            5              0            0    N                 Y   \n",
       "\n",
       "               review_headline  \\\n",
       "0  Great for Windows 7 Laptop!   \n",
       "1                   Five Stars   \n",
       "2                   Five Stars   \n",
       "3                   Five Stars   \n",
       "4                        Great   \n",
       "5                     get this   \n",
       "6                   Five Stars   \n",
       "7         Just what we wanted!   \n",
       "8                    Good deal   \n",
       "9                   Five Stars   \n",
       "\n",
       "                                         review_body review_date  sentiment  \\\n",
       "0  Replaced my Intel Centrino 2230 with the BCM94...  2015-08-31          1   \n",
       "1                              Nice color, I love it  2015-08-31          1   \n",
       "2           Works great to store my pics and videos!  2015-08-31          1   \n",
       "3  Great protector case just sqeeze the sides and...  2015-08-31          1   \n",
       "4  I love this computer. The big screen, the spee...  2015-08-31          1   \n",
       "5     case is well made and keyboard comes in handy.  2015-08-31          1   \n",
       "6                                               Love  2015-08-31          1   \n",
       "7  Fits on my Dad's iPad well. He loves it. It fe...  2015-08-31          1   \n",
       "8  Very happy and totally satisfied with this pur...  2015-08-31          1   \n",
       "9                     Exactly what i needed. Thanks!  2015-08-31          1   \n",
       "\n",
       "  partition_key  \n",
       "0     R5_VPY_VN  \n",
       "1     R5_VPY_VN  \n",
       "2     R5_VPY_VN  \n",
       "3     R5_VPY_VN  \n",
       "4     R5_VPY_VN  \n",
       "5     R5_VPY_VN  \n",
       "6     R5_VPY_VN  \n",
       "7     R5_VPY_VN  \n",
       "8     R5_VPY_VN  \n",
       "9     R5_VPY_VN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_M.show(truncate=20)\n",
    "df_M.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d710d3",
   "metadata": {},
   "source": [
    "Tras haber aplicado el muestreo estratificado a cada una de las particiones válidas, se procede a consolidar los subconjuntos resultantes en un único DataFrame denominado `df_M`, que representa la **muestra final contenida** a utilizar en los modelos de aprendizaje automático.\n",
    "\n",
    "Esta operación se ejecuta mediante el método `consolidate_sampled_partitions()` de la clase `PartitioningManager`, el cual realiza lo siguiente:\n",
    "\n",
    "1. **Unificación de subconjuntos**:\n",
    "   Se utiliza la función `unionByName()` para concatenar horizontalmente todos los DataFrames pertenecientes al diccionario `sampled_partitions`, preservando el esquema original de columnas y asegurando compatibilidad entre las particiones.\n",
    "\n",
    "2. **Validación de la muestra consolidada**:\n",
    "   Una vez construida `df_M`, se imprimen dos distribuciones clave para validar su representatividad:\n",
    "   - **Distribución por clase de sentimiento (`sentiment`)**: refleja que la muestra contiene aproximadamente un 76% de reseñas positivas (2,112,379 registros) y un 24% negativas (654,369 registros). Esta proporción es coherente con la tendencia observada en la población original, donde predominan opiniones favorables.\n",
    "   - **Distribución por clave de partición (`partition_key`)**: evidencia que la muestra incluye contribuciones de prácticamente todas las combinaciones significativas, siendo `R5_VPY_VN` la más dominante con más de 1.47 millones de registros. Otras particiones más pequeñas también están presentes, aunque en menor proporción.\n",
    "\n",
    "3. **Tamaño final de la muestra (`M`)**:\n",
    "   El conjunto consolidado contiene **2,766,748 registros**, lo que representa una fracción manejable del total original (~6.9 millones). \n",
    "   Esta dimensión es adecuada para el entrenamiento de modelos en PySpark, ya que:\n",
    "   - Mantiene la variabilidad de la población.\n",
    "   - Permite ejecutar procesos de modelado con tiempos razonables.\n",
    "   - Conserva la estructura poblacional identificada en el análisis exploratorio.\n",
    "\n",
    "4. **Evaluación de la calidad de la muestra `M`**:\n",
    "   La muestra final puede considerarse de **alta calidad** porque:\n",
    "   - Representa múltiples segmentos de usuarios (por valoración, compra verificada, y participación en Vine).\n",
    "   - Mantiene la proporción natural de clases en cada partición gracias al muestreo estratificado.\n",
    "   - Filtra combinaciones poco frecuentes, lo que evita ruido estadístico y reduce la varianza en modelos posteriores.\n",
    "   - Su tamaño es suficientemente grande como para preservar patrones significativos, pero no excesivo como para dificultar el procesamiento.\n",
    "\n",
    "En conclusión, El DataFrame `df_M` representa una muestra representativa, limpia y balanceada del conjunto original. Es un insumo óptimo para aplicar técnicas de aprendizaje supervisado y no supervisado con alto rendimiento y robustez estadística."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341915cf",
   "metadata": {},
   "source": [
    "# 3) Preparación de los datos\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d941816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingManager:\n",
    "\n",
    "    @staticmethod\n",
    "    def drop_nulls(df):\n",
    "        \"\"\"Drop rows with null values in any column\"\"\"\n",
    "        before = df.count()\n",
    "        df_clean = df.na.drop()\n",
    "        after = df_clean.count()\n",
    "        removed = before - after\n",
    "\n",
    "        print(f\"Removed {removed} rows with null values.\")\n",
    "        if removed > 0:\n",
    "            print(f\"Before: {before} rows, After: {after} rows\")\n",
    "        print(f\"Percentage of rows removed: {removed / before:.2%}\")\n",
    "        print(f\"Percentage of rows remaining: {after / before:.2%}\")\n",
    "        print(f\"Total rows remaining: {after} rows\")\n",
    "        return df.na.drop()\n",
    "\n",
    "    @staticmethod\n",
    "    def cast_column_types(df):\n",
    "        \"\"\"Cast columns to the required types for PySpark ML\"\"\"\n",
    "        return df \\\n",
    "            .withColumn(\"star_rating\", F.col(\"star_rating\").cast(\"int\")) \\\n",
    "            .withColumn(\"helpful_votes\", F.col(\"helpful_votes\").cast(\"int\")) \\\n",
    "            .withColumn(\"total_votes\", F.col(\"total_votes\").cast(\"int\")) \\\n",
    "            .withColumn(\"sentiment\", F.col(\"sentiment\").cast(\"int\"))\n",
    "\n",
    "    @staticmethod\n",
    "    def encode_categorical(df):\n",
    "        \"\"\"Encode categorical columns using StringIndexer\"\"\"\n",
    "        from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "        indexers = [\n",
    "            StringIndexer(inputCol=\"verified_purchase\", outputCol=\"verified_purchase_index\"),\n",
    "            StringIndexer(inputCol=\"vine\", outputCol=\"vine_index\")\n",
    "        ]\n",
    "\n",
    "        for indexer in indexers:\n",
    "            df = indexer.fit(df).transform(df)\n",
    "        \n",
    "        print(\"verified_purchase_index distribution:\")\n",
    "        df.groupBy(\"verified_purchase\", \"verified_purchase_index\") \\\n",
    "            .count().orderBy(\"verified_purchase_index\").show()\n",
    "\n",
    "        print(\"vine_index distribution:\")\n",
    "        df.groupBy(\"vine\", \"vine_index\") \\\n",
    "            .count().orderBy(\"vine_index\").show()\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_outliers(df, numeric_columns=None):\n",
    "        \"\"\"\n",
    "        Detect outliers in numeric columns using the IQR method.\n",
    "        \"\"\"\n",
    "        if not numeric_columns:\n",
    "            from pyspark.sql.types import NumericType\n",
    "            numeric_columns = [f.name for f in df.schema.fields if isinstance(f.dataType, NumericType)]\n",
    "\n",
    "        df_out = df\n",
    "        total_rows = df.count()\n",
    "\n",
    "        print(f\"Total records analyzed: {total_rows}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        for col_name in numeric_columns:\n",
    "            # Compute Q1 & Q3\n",
    "            q1, q3 = df.approxQuantile(col_name, [0.25, 0.75], 0.05)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "            outlier_count = df.filter((F.col(col_name) < lower_bound) | (F.col(col_name) > upper_bound)).count()\n",
    "            outlier_pct = round(outlier_count / total_rows * 100, 2)\n",
    "\n",
    "            print(f\"Variable: {col_name}\")\n",
    "            print(f\" - Q1: {q1}\")\n",
    "            print(f\" - Q3: {q3}\")\n",
    "            print(f\" - IQR: {iqr}\")\n",
    "            print(f\" - Lower bound: {lower_bound}\")\n",
    "            print(f\" - Upper bound: {upper_bound}\")\n",
    "            print(f\" - Outliers detected: {outlier_count} ({outlier_pct}%)\")\n",
    "            print(\"-\"*50)\n",
    "\n",
    "        return df_out\n",
    "\n",
    "    @staticmethod\n",
    "    def assemble_features(df, feature_cols):\n",
    "        \"\"\"Assemble features into a single vector column\"\"\"\n",
    "        from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "        assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "        return assembler.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac41ca19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 219:=========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 rows with null values.\n",
      "Percentage of rows removed: 0.00%\n",
      "Percentage of rows remaining: 100.00%\n",
      "Total rows remaining: 2766748 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_M_clean = PreprocessingManager.drop_nulls(df_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e307d5c2",
   "metadata": {},
   "source": [
    "El primer paso del preprocesamiento consiste en asegurar la integridad estructural de la muestra `M` eliminando aquellos registros que contengan valores nulos en alguna de sus columnas. \n",
    "\n",
    "Para ello, se implementa el método `drop_nulls()` dentro de la clase `PreprocessingManager`, el cual aplica la función `na.drop()` de PySpark sobre el DataFrame `df_M`.\n",
    "\n",
    "Durante este procedimiento, también se calcula y reporta el número total de registros eliminados,  así como el porcentaje que estos representan con respecto al tamaño original de la muestra.\n",
    "\n",
    "El análisis demuestra que el dataset `M` no contiene registros con valores nulos. Por lo tanto, este paso no genera pérdidas de información, lo que indica que la muestra se encuentra en un estado óptimo para las siguientes etapas de preprocesamiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ead0b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_parent: integer (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- star_rating: integer (nullable = true)\n",
      " |-- helpful_votes: integer (nullable = true)\n",
      " |-- total_votes: integer (nullable = true)\n",
      " |-- vine: string (nullable = true)\n",
      " |-- verified_purchase: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: date (nullable = true)\n",
      " |-- sentiment: integer (nullable = true)\n",
      " |-- partition_key: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_M_clean.printSchema()\n",
    "# df_M_casted = PreprocessingManager.cast_column_types(df_M_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9991cb83",
   "metadata": {},
   "source": [
    "Como segundo paso en el preprocesamiento, se revisó la estructura del DataFrame `df_M_clean` para asegurar que las columnas clave cuenten con tipos de datos compatibles con los algoritmos de aprendizaje automático.\n",
    "\n",
    "Al aplicar el método `.printSchema()`, se verificó que:\n",
    "\n",
    "- Las variables numéricas (`star_rating`, `helpful_votes`, `total_votes`, `sentiment`) ya están correctamente tipadas como `integer`.\n",
    "- Las variables categóricas (`verified_purchase`, `vine`) están representadas como `string`, lo cual es apropiado antes de aplicar técnicas de codificación.\n",
    "- No existen columnas relevantes con tipos incompatibles.\n",
    "- La columna `review_date` se encuentra en formato `date`, pero no será utilizada directamente como variable predictiva, por lo que no requiere transformación en esta etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d198df7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_purchase_index distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------------+-------+\n",
      "|verified_purchase|verified_purchase_index|  count|\n",
      "+-----------------+-----------------------+-------+\n",
      "|                Y|                    0.0|2422019|\n",
      "|                N|                    1.0| 344729|\n",
      "+-----------------+-----------------------+-------+\n",
      "\n",
      "vine_index distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------+\n",
      "|vine|vine_index|  count|\n",
      "+----+----------+-------+\n",
      "|   N|       0.0|2752043|\n",
      "|   Y|       1.0|  14705|\n",
      "+----+----------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>verified_purchase_index</th>\n",
       "      <th>vine</th>\n",
       "      <th>vine_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  verified_purchase  verified_purchase_index vine  vine_index\n",
       "0                 Y                      0.0    N         0.0\n",
       "1                 Y                      0.0    N         0.0\n",
       "2                 Y                      0.0    N         0.0\n",
       "3                 Y                      0.0    N         0.0\n",
       "4                 Y                      0.0    N         0.0\n",
       "5                 Y                      0.0    N         0.0\n",
       "6                 Y                      0.0    N         0.0\n",
       "7                 Y                      0.0    N         0.0\n",
       "8                 Y                      0.0    N         0.0\n",
       "9                 Y                      0.0    N         0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_M_encoded = PreprocessingManager.encode_categorical(df_M_clean)\n",
    "df_M_encoded.select(\"verified_purchase\", \"verified_purchase_index\", \"vine\", \"vine_index\").limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e29c5",
   "metadata": {},
   "source": [
    "Como parte del preprocesamiento, se transformaron las variables categóricas `verified_purchase` y `vine` en variables numéricas utilizando la técnica de codificación por índices (`StringIndexer`) provista por PySpark. En otras palabras, se aplicó el método `encode_categorical()` del módulo `PreprocessingManager`, que realiza lo siguiente:\n",
    "- Codifica `verified_purchase` en una nueva columna `verified_purchase_index`.\n",
    "- Codifica `vine` en una nueva columna `vine_index`.\n",
    "- Imprime la distribución de frecuencia de los índices generados, para asegurar que el mapeo es correcto.\n",
    "\n",
    "Este paso es fundamental para que las variables categóricas puedan ser utilizadas como entradas por los algoritmos de aprendizaje automático, ya que estos requieren datos numéricos en su representación vectorial. Cabe destacar que se eligió esta técnica de codificación por las siguientes razones:\n",
    "- **Compatibilidad directa con PySpark ML**: `StringIndexer` es la solución oficial y optimizada para Spark, integrándose fácilmente con otros componentes como `VectorAssembler` y clasificadores como `RandomForestClassifier`, `GBTClassifier` o `MultilayerPerceptronClassifier`.\n",
    "- **Eficiencia computacional**: A diferencia de técnicas como one-hot encoding, no genera columnas adicionales, lo cual es importante cuando se trabaja con grandes volúmenes de datos.\n",
    "- **Adecuado para variables binarias**: Ambas variables (`vine` y `verified_purchase`) son binarias, por lo que no se justifica la complejidad de otros métodos como binary encoding o one-hot.\n",
    "- **Inocuidad del orden numérico**: Los algoritmos seleccionados para este proyecto (basados en árboles y redes) no interpretan los índices como valores ordinales, por lo tanto no existe riesgo de introducir sesgos al asignar 0.0 o 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a1b7696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records analyzed: 2766748\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.=>             (12 + 4) / 16]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alejandrodiazvillagomez/Desktop/Proyecto-Big-Data-PySpark/.venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alejandrodiazvillagomez/Desktop/Proyecto-Big-Data-PySpark/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alejandrodiazvillagomez/.pyenv/versions/3.12.0/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_M_outliers = \u001b[43mPreprocessingManager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetect_outliers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_M_encoded\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mPreprocessingManager.detect_outliers\u001b[39m\u001b[34m(df, numeric_columns)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col_name \u001b[38;5;129;01min\u001b[39;00m numeric_columns:\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# Compute Q1 & Q3\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     q1, q3 = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapproxQuantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.75\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     iqr = q3 - q1\n\u001b[32m     70\u001b[39m     lower_bound = q1 - \u001b[32m1.5\u001b[39m * iqr\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proyecto-Big-Data-PySpark/.venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:4847\u001b[39m, in \u001b[36mDataFrame.approxQuantile\u001b[39m\u001b[34m(self, col, probabilities, relativeError)\u001b[39m\n\u001b[32m   4838\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[32m   4839\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNEGATIVE_VALUE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   4840\u001b[39m         message_parameters={\n\u001b[32m   (...)\u001b[39m\u001b[32m   4843\u001b[39m         },\n\u001b[32m   4844\u001b[39m     )\n\u001b[32m   4845\u001b[39m relativeError = \u001b[38;5;28mfloat\u001b[39m(relativeError)\n\u001b[32m-> \u001b[39m\u001b[32m4847\u001b[39m jaq = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapproxQuantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelativeError\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4848\u001b[39m jaq_list = [\u001b[38;5;28mlist\u001b[39m(j) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m jaq]\n\u001b[32m   4849\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m jaq_list[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m isStr \u001b[38;5;28;01melse\u001b[39;00m jaq_list\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proyecto-Big-Data-PySpark/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proyecto-Big-Data-PySpark/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proyecto-Big-Data-PySpark/.venv/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         answer = smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:-\u001b[32m1\u001b[39m])\n\u001b[32m    512\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_M_outliers = PreprocessingManager.detect_outliers(df_M_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab1ce6",
   "metadata": {},
   "source": [
    "Como parte del preprocesamiento, se aplicó una estrategia de detección de valores atípicos (outliers) a través del método del rango intercuartílico (IQR), sobre las variables numéricas del conjunto `df_M_encoded`.\n",
    "\n",
    "El objetivo de esta etapa es identificar registros con valores extremos que puedan afectar el desempeño de los algoritmos de aprendizaje automático, especialmente aquellos sensibles a escalas de magnitud como redes neuronales o algoritmos no supervisados como K-means.\n",
    "\n",
    "Se analizaron todas las variables numéricas y se documentó el número y proporción de outliers detectados. A continuación, se presentan los hallazgos más relevantes:\n",
    "\n",
    "**Variables sin valores atípicos:**\n",
    "\n",
    "- `customer_id`, `product_parent`, `star_rating`, `sentiment`: sin registros fuera del rango [Q1 - 1.5×IQR, Q3 + 1.5×IQR].\n",
    "  Esto era esperable, dado que:\n",
    "  - `star_rating` y `sentiment` son categóricas discretas (1–5 y 0–1).\n",
    "  - `customer_id` y `product_parent` son identificadores, y no se espera normalidad estadística en ellos.\n",
    "\n",
    "**Variables con valores atípicos detectados:**\n",
    "\n",
    "| Variable         | Outliers detectados | Porcentaje sobre total |\n",
    "|------------------|---------------------|-------------------------|\n",
    "| `helpful_votes`  | 248,715             | 8.99%                   |\n",
    "| `total_votes`    | 354,695             | 12.82%                  |\n",
    "| `verified_purchase_index` | 344,729     | 12.46% (explicado abajo) |\n",
    "| `vine_index`     | 14,705              | 0.53% (por bajo volumen de reseñas Vine) |\n",
    "\n",
    "\n",
    "En primera instancia, los \"outliers\" en `verified_purchase_index` y `vine_index` **no deben eliminarse**, ya que reflejan categorías reales (no errores). Esta detección se debe a que ambas variables están desbalanceadas y presentan un rango intercuartílico igual a cero (IQR = 0), lo que genera falsos positivos en la detección automática.\n",
    "\n",
    "Por otro lado, los outliers detectados en las variables `helpful_votes` y `total_votes` representan casos reales de interacción significativa con las reseñas. Considerando que los algoritmos de aprendizaje a utilizar son robustos frente a este tipo de valores, se decidió **no eliminar los registros extremos**. No obstante, se documentan para un posible análisis futuro y se mantiene su presencia en la muestra M, como reflejo de la variabilidad natural en datos reales de plataformas digitales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41491373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[5.0, 3.0, 4.0, 0.0, 0.0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(5.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(5.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(5.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(5.0, 0.0, 1.0, 0.0, 0.0)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(5.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(5.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(5.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(5.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(5.0, 0.0, 0.0, 0.0, 0.0)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    features  sentiment\n",
       "0  [5.0, 3.0, 4.0, 0.0, 0.0]          1\n",
       "1  (5.0, 0.0, 0.0, 0.0, 0.0)          1\n",
       "2  (5.0, 0.0, 0.0, 0.0, 0.0)          1\n",
       "3  (5.0, 0.0, 0.0, 0.0, 0.0)          1\n",
       "4  (5.0, 0.0, 1.0, 0.0, 0.0)          1\n",
       "5  (5.0, 0.0, 0.0, 0.0, 0.0)          1\n",
       "6  (5.0, 0.0, 0.0, 0.0, 0.0)          1\n",
       "7  (5.0, 0.0, 0.0, 0.0, 0.0)          1\n",
       "8  (5.0, 0.0, 0.0, 0.0, 0.0)          1\n",
       "9  (5.0, 0.0, 0.0, 0.0, 0.0)          1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = [\n",
    "    \"star_rating\",\n",
    "    \"helpful_votes\",\n",
    "    \"total_votes\",\n",
    "    \"verified_purchase_index\",\n",
    "    \"vine_index\"\n",
    "]\n",
    "\n",
    "df_M_ready = PreprocessingManager.assemble_features(df_M_encoded, feature_cols)\n",
    "df_M_ready.select(\"features\", \"sentiment\").limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9026756e",
   "metadata": {},
   "source": [
    "En esta etapa se construye el vector de características requerido por los algoritmos de aprendizaje automático de PySpark. A diferencia de otros frameworks como Scikit-learn, PySpark exige que las variables predictoras estén integradas en una única columna de tipo `Vector` denominada `features`, la cual agrupa de forma estructurada todos los atributos que serán utilizados por los modelos tanto supervisados como no supervisados.\n",
    "\n",
    "Para lograr esta transformación, se empleó el transformador `VectorAssembler`, que recibe como entrada una lista de columnas numéricas y devuelve una columna vectorial combinada. En este caso, se seleccionaron las siguientes variables:\n",
    "\n",
    "| Columna                     | Tipo              | Justificación                                                                 |\n",
    "|----------------------------|-------------------|-------------------------------------------------------------------------------|\n",
    "| `star_rating`              | Numérica           | Refleja la calificación otorgada por el usuario; se correlaciona directamente con el sentimiento. |\n",
    "| `helpful_votes`            | Numérica           | Representa cuántos usuarios consideraron útil una reseña, lo que puede ser indicativo de su impacto. |\n",
    "| `total_votes`              | Numérica           | Mide el alcance de la reseña y complementa a `helpful_votes`.               |\n",
    "| `verified_purchase_index`  | Codificada (0.0/1.0) | Indica si la reseña proviene de una compra verificada; asociado con la credibilidad del comentario. |\n",
    "| `vine_index`               | Codificada (0.0/1.0) | Indica si el usuario participó en el programa Vine, lo que puede influir en la naturaleza de la reseña. |\n",
    "\n",
    "Estas características fueron seleccionadas porque:\n",
    "\n",
    "- Son numéricas o han sido codificadas correctamente como índices.\n",
    "- No presentan valores nulos tras la limpieza.\n",
    "- Representan dimensiones fundamentales del comportamiento del usuario, credibilidad y tipo de participación.\n",
    "- Son robustas y apropiadas para alimentar modelos de clasificación supervisada (como Decision Trees o Random Forests) y agrupamiento no supervisado (como K-Means).\n",
    "\n",
    "La aplicación de este ensamblaje se realizó sobre el conjunto `df_M_encoded`, generando un nuevo DataFrame `df_M_ready` que contiene la columna `features`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411a726",
   "metadata": {},
   "source": [
    "# 4) Preparación del conjunto de entrenamiento y prueba\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3feca74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 267:=========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+----------+\n",
      "|sentiment|  count|percentage|\n",
      "+---------+-------+----------+\n",
      "|        1|2112379|     76.35|\n",
      "|        0| 654369|     23.65|\n",
      "+---------+-------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total = df_M_ready.count()\n",
    "df_M_ready.groupBy(\"sentiment\").count() \\\n",
    "    .withColumn(\"percentage\", F.round(F.col(\"count\") / total * 100, 2)) \\\n",
    "    .orderBy(\"percentage\", ascending=False) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15416562",
   "metadata": {},
   "source": [
    "Una vez finalizado el preprocesamiento de la muestra `M`, se procedió a evaluar la distribución de la variable objetivo `sentiment`, con el fin de identificar posibles desbalances de clase que pudieran afectar el entrenamiento de los modelos (principalmente al modelo de aprendizaje supervisado, debido a su naturaleza).\n",
    "\n",
    "La variable `sentiment` presenta un desbalance moderado, donde la clase positiva (`sentiment` = 1) representa aproximadamente el 76% del total de registros, mientras que la clase negativa (`sentiment` = 0) representa solo el 24% restante. Esta proporción es coherente con la naturaleza del dominio, en el que históricamente se ha observado una tendencia hacia comentarios positivos en plataformas de reseñas como Amazon.\n",
    "\n",
    "Un desbalance de clases puede generar los siguientes riesgos en algoritmos de clasificación:\n",
    "\n",
    "* **Sobreajuste a la clase mayoritaria**: Los modelos tienden a predecir la clase más frecuente, logrando una aparente alta precisión pero bajo rendimiento real en la clase minoritaria.\n",
    "* **Falsas métricas de desempeño**: En contextos desbalanceados, la métrica de accuracy puede ser engañosa, ya que un modelo que siempre predice la clase mayoritaria puede tener una precisión elevada pero carecer de utilidad práctica.\n",
    "* **Pérdida de sensibilidad hacia clases minoritarias**: Se reduce la capacidad del modelo para detectar patrones que expliquen los comentarios negativos (clase minoritaria), que son precisamente los más relevantes para la toma de decisiones correctivas.\n",
    "\n",
    "Cabe destacar que este desbalance fue mitigado desde la etapa de particionado y muestreo estratificado:\n",
    "* Al aplicar la función `stratified_sample_partitioned_data()`, se garantizó que cada partición mantuviera proporcionalmente representadas ambas clases (`sentiment` = 0 y `sentiment` = 1), en función de una fracción fija por clase.\n",
    "* Esto permitió construir una muestra `M` contenida pero estructuralmente coherente, respetando la proporción natural de sentimientos en cada subconjunto y evitando que el desbalance se amplificara en las etapas posteriores.\n",
    "\n",
    "En otras palabras, aunque la clase positiva sigue siendo mayoritaria en la muestra final, el diseño metodológico aplicado (basado en particionamiento por variables de caracterización y muestreo estratificado) permite conservar la representatividad del conjunto original sin incurrir en sesgos extremos.\n",
    "\n",
    "Sin embargo, es importante menciona que para compensar este desbalance moderado en fases posteriores, se recomienda evaluar los modelos mediante métricas como F1-score, precision y recall, particularmente en la clase minoritaria. O bien, considerar ajustes adicionales, como ponderación de clases o técnicas de resampling, si el desempeño sobre la clase negativa no resulta satisfactorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "419352b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de entrenamiento:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+----------+\n",
      "|sentiment|  count|percentage|\n",
      "+---------+-------+----------+\n",
      "|        0| 523456|     23.65|\n",
      "|        1|1690239|     76.35|\n",
      "+---------+-------+----------+\n",
      "\n",
      "Conjunto de prueba:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 279:=========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+----------+\n",
      "|sentiment| count|percentage|\n",
      "+---------+------+----------+\n",
      "|        0|130913|     23.67|\n",
      "|        1|422140|     76.33|\n",
      "+---------+------+----------+\n",
      "\n",
      "Tamaño del conjunto de entrenamiento: 2213695 registros (80.01%)\n",
      "Tamaño del conjunto de prueba: 553053 registros (19.99%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.8  # 80% para entrenamiento, 20% para prueba\n",
    "\n",
    "# División estratificada para mantener la proporción de clases\n",
    "train_df, test_df = df_M_ready.randomSplit([train_ratio, 1 - train_ratio], seed=42)\n",
    "\n",
    "# Verificación de las proporciones en cada conjunto\n",
    "print(\"Conjunto de entrenamiento:\")\n",
    "train_total = train_df.count()\n",
    "train_df.groupBy(\"sentiment\").count() \\\n",
    "    .withColumn(\"percentage\", F.round(F.col(\"count\") / train_total * 100, 2)) \\\n",
    "    .orderBy(\"sentiment\").show()\n",
    "\n",
    "print(\"Conjunto de prueba:\")\n",
    "test_total = test_df.count()\n",
    "test_df.groupBy(\"sentiment\").count() \\\n",
    "    .withColumn(\"percentage\", F.round(F.col(\"count\") / test_total * 100, 2)) \\\n",
    "    .orderBy(\"sentiment\").show()\n",
    "\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {train_total} registros ({round(train_total/total*100, 2)}%)\")\n",
    "print(f\"Tamaño del conjunto de prueba: {test_total} registros ({round(test_total/total*100, 2)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da64b3",
   "metadata": {},
   "source": [
    "Para la preparación de los conjuntos de entrenamiento y prueba, se implementó una división siguiendo estos principios:\n",
    "\n",
    "1. **Proporción de división 80/20**: Se asignó el 80% de los datos al conjunto de entrenamiento y el 20% al conjunto de prueba. Esta proporción es óptima porque:\n",
    "   - Proporciona suficientes datos para entrenar modelos complejos (2.2M registros aproximadamente)\n",
    "   - Mantiene un conjunto de prueba robusto (~550K registros) para evaluación confiable\n",
    "   - Es una proporción estándar recomendada en la literatura para conjuntos grandes\n",
    "\n",
    "2. **Técnica de muestreo aleatorio estratificado**: Se utilizó el método `randomSplit()` con una semilla aleatoria fija (seed=42) para garantizar:\n",
    "   - Reproducibilidad de los resultados\n",
    "   - Mantenimiento de las proporciones de la variable objetivo en ambos conjuntos\n",
    "   - Minimización del riesgo de sesgo de selección\n",
    "\n",
    "Esta estrategia de división es coherente con las mejores prácticas en aprendizaje automático y adecuada para el problema de clasificación binaria de sentimientos, donde es crucial mantener la representatividad de ambas clases en los conjuntos de entrenamiento y prueba.\n",
    "\n",
    "---\n",
    "\n",
    "⚠️ *Nota técnica sobre estratificación*\n",
    "\n",
    "Aunque el método `randomSplit()` de PySpark no realiza una estratificación explícita como lo haría `train_test_split(..., stratify=...)` en bibliotecas como `scikit-learn`, en conjuntos de datos de gran tamaño, como el que se maneja en este proyecto (más de 2.7 millones de registros), la distribución de clases tiende a conservarse de forma natural en cada subconjunto.\n",
    "\n",
    "Esta hipótesis ha sido verificada empíricamente en la presente implementación al comparar las proporciones de la variable `sentiment` antes y después de la partición, observando una correspondencia muy cercana entre ambas.\n",
    "\n",
    "Por ello, se considera que esta aproximación es válida, eficiente y suficiente para los fines del análisis y modelado, sin necesidad de una estratificación adicional más costosa computacionalmente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af3a94b",
   "metadata": {},
   "source": [
    "# 5) Construcción de modelos de aprendizaje supervisado y no supervisado\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeeeab7",
   "metadata": {},
   "source": [
    "## 5.1) Algoritmo de aprendizaje supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e038f",
   "metadata": {},
   "source": [
    "### 5.1.1) Modelo con sobreajuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1875b671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo RandomForestClassifier entrenado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Definición del modelo\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"sentiment\",\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"prediction\",\n",
    "    probabilityCol=\"probability\",\n",
    "    seed=42,\n",
    "    numTrees=100,\n",
    "    maxDepth=10\n",
    ")\n",
    "\n",
    "# Entrenamiento\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "print(\"Modelo RandomForestClassifier entrenado exitosamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b935d45",
   "metadata": {},
   "source": [
    "Para abordar la tarea de clasificación binaria del sentimiento (positivo o negativo) en las reseñas de productos, se seleccionó el algoritmo `RandomForestClassifier` del módulo `pyspark.ml.classification` como modelo de aprendizaje supervisado. Esta elección fue sustentada tanto en criterios técnicos como en consideraciones prácticas vinculadas a la naturaleza del problema y las características del conjunto de datos.\n",
    "\n",
    "El modelo en cuestión es una técnica de ensamblado basada en árboles de decisión que ofrece una excelente relación entre precisión, robustez y eficiencia computacional, lo cual lo hace especialmente adecuado para contextos de Big Data como el presente. Las razones principales de su elección se detallan a continuación:\n",
    "\n",
    "* **Adecuado para clasificación binaria**: El problema se define como una clasificación binaria (`sentiment` = 0 o 1), una tarea donde Random Forest ha demostrado un desempeño sobresaliente en múltiples estudios y aplicaciones prácticas.\n",
    "* **Manejo eficiente de datos numéricos y categóricos codificados**: Todas las variables seleccionadas (`star_rating`, `helpful_votes`, `total_votes`, `verified_purchase_index`, `vine_index`) son de tipo numérico o han sido codificadas previamente mediante `StringIndexer`, lo cual es completamente compatible con este algoritmo sin necesidad de transformaciones adicionales.\n",
    "* **Robustez frente a outliers y datos ruidosos**: Random Forest no se ve significativamente afectado por valores atípicos o ruido en los datos, como aquellos observados en las variables `helpful_votes` y `total_votes`, por lo que evita la necesidad de aplicar estrategias de eliminación o transformación adicional de estos valores.\n",
    "* **No requiere normalización de variables**: A diferencia de algoritmos como la regresión logística o las redes neuronales, Random Forest no depende de la escala de las variables de entrada, lo que reduce el número de pasos de preprocesamiento requeridos.\n",
    "* **Capacidad para manejar datos desbalanceados**: Aunque el dataset presenta un leve desbalance de clases (76% positivas, 24% negativas), Random Forest puede mitigar este efecto mediante su mecanismo de muestreo aleatorio (bootstrap sampling) en cada árbol.\n",
    "* **Interpretabilidad y análisis de importancia de variables**: Una ventaja adicional del modelo es su capacidad para generar métricas de importancia de características, lo cual es valioso para interpretar qué variables contribuyen más a la predicción del sentimiento.\n",
    "**Costo computacional razonable**: Dado que el entrenamiento se realiza en un entorno distribuido con PySpark, el uso de `RandomForestClassifier` permite aprovechar eficientemente los recursos disponibles, obteniendo buenos resultados sin incurrir en el alto costo computacional que requerirían otros modelos como `GBTClassifier` o `MultilayerPerceptronClassifier`.\n",
    "\n",
    "#### **Comparación con otros modelos**\n",
    "| Modelo                 | Motivo por el cual no fue seleccionado                                     |\n",
    "| ---------------------- | -------------------------------------------------------------------------- |\n",
    "| `LogisticRegression`   | Requiere normalización y es más sensible a desbalance de clases.           |\n",
    "| `MultilayerPerceptron` | Demanda tuning de hiperparámetros, normalización y mayor poder de cómputo. |\n",
    "| `GBTClassifier`        | Más preciso en algunos casos, pero considerablemente más costoso.          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3496edd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 315:=========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Precision (ponderada): 1.0000\n",
      "Recall (ponderado): 1.0000\n",
      "F1 Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Realizamos predicciones sobre el conjunto de prueba\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "\n",
    "# Definimos evaluadores para distintas métricas\n",
    "rf_evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"sentiment\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rf_evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"sentiment\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "rf_evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"sentiment\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "rf_evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"sentiment\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# Calculamos las métricas\n",
    "rf_accuracy = rf_evaluator_accuracy.evaluate(rf_predictions)\n",
    "rf_precision = rf_evaluator_precision.evaluate(rf_predictions)\n",
    "rf_recall = rf_evaluator_recall.evaluate(rf_predictions)\n",
    "rf_f1 = rf_evaluator_f1.evaluate(rf_predictions)\n",
    "\n",
    "# Extraemos las importancias de las características\n",
    "rf_importances = rf_model.featureImportances.toArray()\n",
    "\n",
    "# Mostramos los resultados\n",
    "print(\"Resultados de la evaluación del modelo:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"Precision (ponderada): {rf_precision:.4f}\")\n",
    "print(f\"Recall (ponderado): {rf_recall:.4f}\")\n",
    "print(f\"F1 Score: {rf_f1:.4f}\")\n",
    "\n",
    "print(\"\\nImportancia de las características:\")\n",
    "print(\"=\"*60)\n",
    "for feature, importance in zip(feature_cols, rf_importances):\n",
    "    print(f\" - {feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e2a98",
   "metadata": {},
   "source": [
    "Durante la evaluación del modelo de clasificación supervisada utilizando un Random Forest Classifier, se obtuvieron resultados inusualmente perfectos. Estos resultados, aunque a primera vista parecen excelentes, no son realistas en contextos de datos reales, y constituyen un claro indicio de sobreajuste (**overfitting**). Esto significa que el modelo ha memorizado los patrones exactos de los datos de entrenamiento en lugar de aprender generalizaciones útiles.\n",
    "\n",
    "Al analizar las importancias de las características (`featureImportances`) reportadas por el modelo, se observó que la variable `star_rating` domina completamente el proceso de decisión del modelo, acumulando el 97.3% del peso total en la predicción. Dado que tanto `star_rating` como la variable objetivo `sentiment` provienen del dataset original y no fueron calculadas por separado, es altamente probable que exista una relación directa o redundante entre ambas variables, lo que explicaría la capacidad del modelo para predecir el sentimiento con precisión perfecta. Esta situación evidencia una fuga de información (**data leakage**), donde el modelo está utilizando indirectamente la etiqueta como una entrada predictiva, anulando su capacidad de generalización.\n",
    "\n",
    "Como mejora inmediata, se propone reentrenar el modelo excluyendo la variable `star_rating` del conjunto de características, con el objetivo de evaluar su verdadero desempeño al depender de señales más sutiles y no redundantes.\n",
    "\n",
    "Este ajuste permite:\n",
    "* Reducir el riesgo de sobreajuste.\n",
    "* Evaluar si otras variables como `helpful_votes`, `total_votes`, `verified_purchase_index` y `vine_index` aportan valor predictivo genuino.\n",
    "* Obtener métricas de evaluación más realistas y útiles para el análisis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345528a0",
   "metadata": {},
   "source": [
    "### 5.1.2) Modelo sin sobreajuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33bac5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "feature_cols_v2 = [\n",
    "    \"helpful_votes\",\n",
    "    \"total_votes\",\n",
    "    \"verified_purchase_index\",\n",
    "    \"vine_index\"\n",
    "]\n",
    "\n",
    "df_M_ready_v2 = PreprocessingManager.assemble_features(df_M_encoded, feature_cols_v2)\n",
    "df_M_ready_v2.select(\"features\", \"sentiment\").limit(10).toPandas()\n",
    "\n",
    "train_ratio = 0.8  # 80% para entrenamiento, 20% para prueba\n",
    "train_df_v2, test_df_v2 = df_M_ready_v2.randomSplit([train_ratio, 1 - train_ratio], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6725d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.==========>    (11 + 5) / 16]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alejandrodiazvillagomez/Desktop/Proyecto-Big-Data-PySpark/.venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alejandrodiazvillagomez/Desktop/Proyecto-Big-Data-PySpark/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/alejandrodiazvillagomez/.pyenv/versions/3.12.0/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m rf_v2 = RandomForestClassifier(\n\u001b[32m      2\u001b[39m     labelCol=\u001b[33m\"\u001b[39m\u001b[33msentiment\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     featuresCol=\u001b[33m\"\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     maxDepth=\u001b[32m10\u001b[39m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Entrenamiento\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m rf_model_v2 = \u001b[43mrf_v2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df_v2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModelo RandomForestClassifier entrenado exitosamente.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proyecto-Big-Data-PySpark/.venv/lib/python3.12/site-packages/pyspark/ml/base.py:205\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    208\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    209\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    210\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proyecto-Big-Data-PySpark/.venv/lib/python3.12/site-packages/pyspark/ml/wrapper.py:381\u001b[39m, in \u001b[36mJavaEstimator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) -> JM:\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m     java_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._create_model(java_model)\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copyValues(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proyecto-Big-Data-PySpark/.venv/lib/python3.12/site-packages/pyspark/ml/wrapper.py:378\u001b[39m, in \u001b[36mJavaEstimator._fit_java\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proyecto-Big-Data-PySpark/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proyecto-Big-Data-PySpark/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proyecto-Big-Data-PySpark/.venv/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         answer = smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:-\u001b[32m1\u001b[39m])\n\u001b[32m    512\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.0/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rf_v2 = RandomForestClassifier(\n",
    "    labelCol=\"sentiment\",\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"prediction\",\n",
    "    probabilityCol=\"probability\",\n",
    "    seed=42,\n",
    "    numTrees=100,\n",
    "    maxDepth=10\n",
    ")\n",
    "\n",
    "# Entrenamiento\n",
    "rf_model_v2 = rf_v2.fit(train_df_v2)\n",
    "\n",
    "print(\"Modelo RandomForestClassifier entrenado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a5acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions_v2 = rf_model_v2.transform(test_df_v2)\n",
    "\n",
    "rf_evaluator_accuracy_v2 = MulticlassClassificationEvaluator(labelCol=\"sentiment\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rf_evaluator_precision_v2 = MulticlassClassificationEvaluator(labelCol=\"sentiment\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "rf_evaluator_recall_v2 = MulticlassClassificationEvaluator(labelCol=\"sentiment\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "rf_evaluator_f1_v2 = MulticlassClassificationEvaluator(labelCol=\"sentiment\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "rf_accuracy_v2 = rf_evaluator_accuracy_v2.evaluate(rf_predictions_v2)\n",
    "rf_precision_v2 = rf_evaluator_precision_v2.evaluate(rf_predictions_v2)\n",
    "rf_recall_v2 = rf_evaluator_recall_v2.evaluate(rf_predictions_v2)\n",
    "rf_f1_v2 = rf_evaluator_f1_v2.evaluate(rf_predictions_v2)\n",
    "\n",
    "rf_importances_v2 = rf_model_v2.featureImportances.toArray()\n",
    "\n",
    "print(\"Resultados de la evaluación del modelo:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {rf_accuracy_v2:.4f}\")\n",
    "print(f\"Precision (ponderada): {rf_precision_v2:.4f}\")\n",
    "print(f\"Recall (ponderado): {rf_recall_v2:.4f}\")\n",
    "print(f\"F1 Score: {rf_f1_v2:.4f}\")\n",
    "\n",
    "print(\"\\nImportancia de las características:\")\n",
    "print(\"=\"*60)\n",
    "for feature, importance in zip(feature_cols_v2, rf_importances_v2):\n",
    "    print(f\" - {feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85b141",
   "metadata": {},
   "source": [
    "## 5.2) Algoritmo de aprendizaje no supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f9cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Definición del modelo KMeans\n",
    "kmeans = KMeans(\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"prediction\",\n",
    "    k=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "kmeans_model = kmeans.fit(train_df)\n",
    "\n",
    "print(\"Modelo KMeans entrenado y aplicado exitosamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd215ee",
   "metadata": {},
   "source": [
    "Para la etapa de aprendizaje no supervisado, se selecciona el algoritmo `K-Means Clustering` como el método más adecuado para llevar a cabo un análisis de agrupamiento sobre el conjunto de entrenamiento `train_df`.\n",
    "\n",
    "K-Means es un algoritmo ampliamente utilizado en problemas de agrupamiento debido a su eficiencia computacional, simplicidad conceptual y buena escalabilidad para grandes volúmenes de datos, lo cual es especialmente importante en contextos de Big Data como el presente.\n",
    "\n",
    "Las justificaciones de la elección de este modelo son:\n",
    "\n",
    "* **Escalabilidad**: K-Means tiene una complejidad computacional baja (O(n·k·i)), lo que lo hace ideal para trabajar con conjuntos de datos de millones de registros como en el caso actual.\n",
    "* **Compatibilidad con PySpark**: PySpark MLlib ofrece una implementación altamente optimizada de KMeans que se integra fácilmente con pipelines de transformación (`VectorAssembler`) y evaluación (`ClusteringEvaluator`).\n",
    "* **Simplicidad interpretativa**: Los centroides obtenidos permiten caracterizar los grupos descubiertos con base en las variables seleccionadas, facilitando el análisis exploratorio.\n",
    "\n",
    "Los hiperparámetros seleccionados son:\n",
    "\n",
    "| Hiperparámetro  | Valor        | Justificación                                                                                                 |\n",
    "| --------------- | ------------ | ------------------------------------------------------------------------------------------------------------- |\n",
    "| `k`             | 5            | Valor inicial común para identificar segmentos diferenciados. Se evaluará con métricas internas para refinar. |\n",
    "| `seed`          | 42           | Para garantizar reproducibilidad de los resultados.                                                           |\n",
    "| `featuresCol`   | `features`   | Define las columnas de entrada para el agrupamiento.                                                          |\n",
    "| `predictionCol` | `prediction` | Almacena la asignación de cluster de cada observación.                                                        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1636ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Predicción de clústeres sobre el conjunto de entrenamiento\n",
    "kmeans_predictions = kmeans_model.transform(train_df)\n",
    "\n",
    "# kmeans_predictions.select(\"prediction\", \"features\").show(10, truncate=False)\n",
    "\n",
    "# Evaluador de agrupamiento basado en distancia euclidiana\n",
    "evaluator = ClusteringEvaluator(\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"silhouette\",\n",
    "    distanceMeasure=\"squaredEuclidean\"\n",
    ")\n",
    "\n",
    "# Cálculo del Silhouette Score\n",
    "silhouette_score = evaluator.evaluate(kmeans_predictions)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62b3923",
   "metadata": {},
   "source": [
    "Para evaluar el modelo KMeans en PySpark, el criterio más comúnmente aceptado es el **Silhouette Score**, ya que mide qué tan bien se agrupan los datos dentro de sus clústeres. PySpark lo proporciona a través del evaluador `ClusteringEvaluator`.\n",
    "\n",
    "El Silhouette Score varía entre -1 y 1:\n",
    "* Cercano a 1: buena cohesión intra-clúster y buena separación entre clústeres.\n",
    "* Cercano a 0: clústeres se superponen, no hay separación clara.\n",
    "* Negativo: mala asignación, puntos estarían mejor en otros clústeres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bbb03b",
   "metadata": {},
   "source": [
    "## 5.3) Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb321711",
   "metadata": {},
   "source": [
    "Resultados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
