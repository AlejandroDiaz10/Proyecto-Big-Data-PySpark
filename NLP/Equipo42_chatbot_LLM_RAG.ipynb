{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hVND8xY2OKY"
      },
      "source": [
        "# **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "## Maestría en Inteligencia Artificial Aplicada\n",
        "#### Tecnológico de Monterrey\n",
        "#### Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "### **Adtividad en Equipos: sistema LLM + RAG**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aimHVFOv23lm"
      },
      "source": [
        "#### **Nombres y matrículas de los integrantes del equipo:**\n",
        "\n",
        "**Equipo 42**\n",
        "\n",
        "|  NOMBRE COMPLETO             |     MATRÍCULA     | \n",
        "| :--------------------------: |:-----------------:|\n",
        "| Alejandro Díaz Villagómez    |  A01276769        | \n",
        "| Emiliano Saucedo Arriola     |  A01659258        | \n",
        "| Javier Adrian Villa León     |  A01242469        | \n",
        "| Karina Zecua Cruz            |  A01795651        | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jimvsiVgjMg"
      },
      "source": [
        "* ##### **El formato de este cuaderno de Jupyter es libre, pero debe incuir al menos las siguientes secciones:**\n",
        "\n",
        "  * ##### **Introducción de la problemática a resolver.**\n",
        "  * ##### **Sistema RAG + LLM**\n",
        "  * ##### **El chatbot, incluyendo ejemplos de prueba.**\n",
        "  * ##### **Conclusiones**\n",
        "\n",
        "* ##### **Pueden importar los paquetes o librerías que requieran.**\n",
        "\n",
        "* ##### **Pueden incluir las celdas y líneas de código que deseen.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Problemática a resolver**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Introducción y justificación donde se describa la temática elegida y la\n",
        "problemática que el chatbot busca resolver o apoyar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Sistema RAG + LLM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Informe documentando de la manera en que funciona el chatbot con un sistema RAG + LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Chatbot (con ejemplos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "ollama run gemma3:1b\n",
        "```\n",
        "\n",
        "```\n",
        "ollama run gemma2:2b\n",
        "```\n",
        "\n",
        "```\n",
        "/bye\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bfs5Zxc9j7Uf"
      },
      "outputs": [],
      "source": [
        "# Instalaciones necesarias\n",
        "# !pip install langchain langchain-community langchain-ollama chromadb sentence-transformers faiss-cpu pypdf python-docx gradio\n",
        "# !pip freeze > requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "import gradio as gr\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader,\n",
        "    TextLoader,\n",
        "    DirectoryLoader,\n",
        "    Docx2txtLoader,\n",
        ")\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_community.vectorstores import FAISS, Chroma\n",
        "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DocumentLoader:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def load(self, paths: List[str]) -> List[Document]:\n",
        "        all_docs = []\n",
        "        for path in paths:\n",
        "            path_obj = Path(path)\n",
        "            if path_obj.is_file():\n",
        "                all_docs.extend(self._load_file(str(path_obj)))\n",
        "            elif path_obj.is_dir():\n",
        "                all_docs.extend(self._load_dir(str(path_obj)))\n",
        "        return all_docs\n",
        "\n",
        "    def _load_file(self, file_path: str) -> List[Document]:\n",
        "        ext = Path(file_path).suffix.lower()\n",
        "        loader = {\n",
        "            \".pdf\": PyPDFLoader,\n",
        "            \".txt\": lambda p: TextLoader(p, encoding=\"utf-8\"),\n",
        "            \".docx\": Docx2txtLoader,\n",
        "            \".doc\": Docx2txtLoader,\n",
        "        }.get(ext, lambda p: TextLoader(p, encoding=\"utf-8\"))\n",
        "        return loader(file_path).load()\n",
        "\n",
        "    def _load_dir(self, dir_path: str) -> List[Document]:\n",
        "        docs = []\n",
        "        for ext, loader_cls in {\n",
        "            \"**/*.pdf\": PyPDFLoader,\n",
        "            \"**/*.txt\": TextLoader,\n",
        "            \"**/*.docx\": Docx2txtLoader,\n",
        "        }.items():\n",
        "            try:\n",
        "                loader = DirectoryLoader(dir_path, glob=ext, loader_cls=loader_cls)\n",
        "                docs.extend(loader.load())\n",
        "            except:\n",
        "                pass\n",
        "        return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VectorStoreManager:\n",
        "    def __init__(\n",
        "        self, embedding_model, db_type=\"faiss\", chunk_size=1000, chunk_overlap=200\n",
        "    ):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.db_type = db_type\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.vector_store = None\n",
        "        self.retriever = None\n",
        "\n",
        "    def process_and_store(\n",
        "        self, documents: List[Document], persist_path: Optional[str] = None\n",
        "    ):\n",
        "        splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=self.chunk_size,\n",
        "            chunk_overlap=self.chunk_overlap,\n",
        "            length_function=len,\n",
        "        )\n",
        "        chunks = splitter.split_documents(documents)\n",
        "        if self.db_type == \"faiss\":\n",
        "            self.vector_store = FAISS.from_documents(chunks, self.embedding_model)\n",
        "        elif self.db_type == \"chroma\":\n",
        "            self.vector_store = Chroma.from_documents(\n",
        "                chunks, self.embedding_model, persist_directory=persist_path\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Tipo de vector store no soportado\")\n",
        "        self.retriever = self.vector_store.as_retriever(search_kwargs={\"k\": 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RAGChatbot:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name=\"gemma3:1b\",\n",
        "        embedding_model=\"gemma2:2b\",\n",
        "        db_type=\"faiss\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.llm = OllamaLLM(\n",
        "            model=model_name, \n",
        "            temperature=kwargs.get(\"temperature\", 0.3)\n",
        "        )\n",
        "        self.embeddings = OllamaEmbeddings(model=embedding_model)\n",
        "        self.memory = ConversationBufferWindowMemory(\n",
        "            k=5, \n",
        "            return_messages=True, \n",
        "            memory_key=\"chat_history\", \n",
        "            output_key=\"answer\"\n",
        "        )\n",
        "        self.loader = DocumentLoader()\n",
        "        self.vector_mgr = VectorStoreManager(\n",
        "            self.embeddings, \n",
        "            db_type=db_type, \n",
        "            **kwargs\n",
        "        )\n",
        "        self.qa_chain = None\n",
        "\n",
        "        self.prompt = PromptTemplate(\n",
        "            template=self._default_prompt(),\n",
        "            input_variables=[\"context\", \"question\", \"chat_history\"],\n",
        "        )\n",
        "\n",
        "    def setup(self, document_paths: List[str], persist_dir: Optional[str] = None):\n",
        "        documents = self.loader.load(document_paths)\n",
        "        self.vector_mgr.process_and_store(documents, persist_dir)\n",
        "        self.setup_qa_chain()\n",
        "\n",
        "    def setup_qa_chain(self):\n",
        "        if not self.vector_mgr.retriever:\n",
        "            raise ValueError(\"El retriever no está configurado\")\n",
        "        \n",
        "        self.qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=self.llm,\n",
        "            retriever=self.vector_mgr.retriever,\n",
        "            memory=self.memory,\n",
        "            return_source_documents=True,\n",
        "            combine_docs_chain_kwargs={\"prompt\": self.prompt},\n",
        "        )\n",
        "\n",
        "    def ask(self, question: str) -> Dict[str, Any]:\n",
        "        if not self.qa_chain:\n",
        "            raise ValueError(\"La cadena QA no está configurada\")\n",
        "        try:\n",
        "            result = self.qa_chain.invoke({\"question\": question})\n",
        "            return {\n",
        "                \"question\": question,\n",
        "                \"answer\": result[\"answer\"],\n",
        "                \"sources\": [\n",
        "                    doc.metadata.get(\"source\", \"Unknown\")\n",
        "                    for doc in result.get(\"source_documents\", [])\n",
        "                ],\n",
        "                \"source_documents\": result.get(\"source_documents\", []),\n",
        "            }\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "\n",
        "            traceback.print_exc()\n",
        "            return {\n",
        "                \"question\": question,\n",
        "                \"answer\": f\"Error al procesar la pregunta: {str(e)}\",\n",
        "                \"sources\": [],\n",
        "                \"source_documents\": [],\n",
        "            }\n",
        "\n",
        "    def export_chat_history(self, json_path=\"chat_history/json_format.json\"):\n",
        "        os.makedirs(Path(json_path).parent, exist_ok=True)  # Crear carpeta si no existe\n",
        "        chat = self.memory.chat_memory.messages\n",
        "        json_data = [{\"role\": msg.type, \"content\": msg.content} for msg in chat]\n",
        "\n",
        "        with open(json_path, \"w\", encoding=\"utf-8\") as jf:\n",
        "            json.dump(json_data, jf, ensure_ascii=False, indent=2)\n",
        "\n",
        "        return f\"📝 Historial exportado a: {Path(json_path).resolve()}\"\n",
        "\n",
        "    def _default_prompt(self):\n",
        "        return \"\"\"\n",
        "            Eres un asistente útil que responde preguntas basadas en el contexto proporcionado y el historial de la conversación.\n",
        "            Responde siempre en español.\n",
        "\n",
        "            Historial de Conversación:\n",
        "            {chat_history}\n",
        "\n",
        "            Contexto del Documento:\n",
        "            {context}\n",
        "\n",
        "            Pregunta:\n",
        "            {question}\n",
        "\n",
        "            Respuesta útil en español:\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/zb/tl73nj3d2933b0zx9fk5tfbm0000gn/T/ipykernel_17207/896455901.py:14: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  self.memory = ConversationBufferWindowMemory(\n"
          ]
        }
      ],
      "source": [
        "# Ruta a los documentos\n",
        "document_paths = [\"./documents\"]\n",
        "\n",
        "chatbot = RAGChatbot(\n",
        "    model_name=\"gemma3:1b\",\n",
        "    embedding_model=\"gemma2:2b\",\n",
        "    db_type=\"faiss\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        ")\n",
        "\n",
        "# Configurar el sistema completo\n",
        "chatbot.setup(document_paths=document_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 Respuesta: El documento trata sobre el Proyecto 'Titán', que es una iniciativa de desarrollo de software para la gestión de clientes (CRM). Fue aprobado en la reunión del Q3 de 2024 y sus características principales son: Módulo de Autenticación (con OAuth 2.0 y 2FA), Módulo de Clientes (con historial de interacciones), y Módulo de Reportes (generación de informes de ventas mensuales en PDF y CSV). El líder técnico es Ana García y el equipo de desarrollo incluye Juan Pérez (Backend) y María López (Frontend). El presupuesto asignado para el MVP es de $50,000 USD.\n",
            "\n",
            "📚 Fuentes: ['documents/sample.txt']\n"
          ]
        }
      ],
      "source": [
        "respuesta = chatbot.ask(\"¿Qué información contiene el documento?\")\n",
        "print(\"🧠 Respuesta:\", respuesta[\"answer\"])\n",
        "print(\"📚 Fuentes:\", respuesta[\"sources\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 Respuesta: Mi pregunta anterior era: ¿Qué información contiene el documento?\n",
            "\n",
            "📚 Fuentes: ['documents/sample.txt']\n"
          ]
        }
      ],
      "source": [
        "respuesta = chatbot.ask(\"Te acuerdas cuál fue mi pregunta anterior?\")\n",
        "print(\"🧠 Respuesta:\", respuesta[\"answer\"])\n",
        "print(\"📚 Fuentes:\", respuesta[\"sources\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 Respuesta: El líder del proyecto era Ana García.\n",
            "📚 Fuentes: ['documents/sample.txt']\n"
          ]
        }
      ],
      "source": [
        "respuesta = chatbot.ask(\"Y quién era el líder del proyecto?\")\n",
        "print(\"🧠 Respuesta:\", respuesta[\"answer\"])\n",
        "print(\"📚 Fuentes:\", respuesta[\"sources\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 Respuesta: El líder del proyecto era Ana García.\n",
            "📚 Fuentes: ['documents/sample.txt']\n"
          ]
        }
      ],
      "source": [
        "respuesta = chatbot.ask(\"Te acuerdas cuál fue tu última respuesta?\")\n",
        "print(\"🧠 Respuesta:\", respuesta[\"answer\"])\n",
        "print(\"📚 Fuentes:\", respuesta[\"sources\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 Respuesta: Google es una empresa tecnológica multinacional con sede en Silicon Valley, que se especializa en búsqueda en línea, sistemas operativos, herramientas de productividad y computación en la nube. Fundada en 1996, Google es una de las empresas más valiosas del mundo.\n",
            "\n",
            "📚 Fuentes: ['documents/sample.txt']\n"
          ]
        }
      ],
      "source": [
        "respuesta = chatbot.ask(\"Qué es Google?\")\n",
        "print(\"🧠 Respuesta:\", respuesta[\"answer\"])\n",
        "print(\"📚 Fuentes:\", respuesta[\"sources\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Gradio**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradio_interface(chatbot: RAGChatbot):\n",
        "    def respond(message, history):\n",
        "        response = chatbot.ask(message)\n",
        "        return response[\"answer\"]\n",
        "\n",
        "    # Esta función ahora solo llama a export_chat_history y el gr.Info() se encargará del toast\n",
        "    def export_chat_action():\n",
        "        try:\n",
        "            message = chatbot.export_chat_history()\n",
        "            gr.Info(message)\n",
        "        except Exception as e:\n",
        "            error_message = f\"Error al exportar historial: {str(e)}\"\n",
        "            gr.Warning(error_message)\n",
        "        return \"\"\n",
        "\n",
        "    # Definimos un tema personalizado para Gradio\n",
        "    custom_theme = gr.themes.Soft(\n",
        "        primary_hue=\"purple\",\n",
        "        secondary_hue=\"cyan\",\n",
        "        neutral_hue=\"gray\",\n",
        "    )\n",
        "\n",
        "    with gr.Blocks(theme=custom_theme) as demo:\n",
        "        # Inyectamos CSS para asegurar que el texto sea legible\n",
        "        gr.HTML(\n",
        "            \"\"\"\n",
        "            <style>\n",
        "                body { background-color: var(--background-fill-primary); }\n",
        "                .gradio-container { background-color: var(--background-fill-primary); }\n",
        "                .message.user .wrap {\n",
        "                    background-color: var(--chatbot-bubble-user-background-color) !important;\n",
        "                    color: black !important;\n",
        "                }\n",
        "                .message.bot .wrap {\n",
        "                    background-color: var(--chatbot-bubble-bot-background-color) !important;\n",
        "                    color: black !important;\n",
        "                }\n",
        "\n",
        "                .gr-form input[type=\"text\"], .gr-form textarea {\n",
        "                    color: black !important;\n",
        "                }\n",
        "            </style>\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            # 📚 Chatbot RAG con Memoria\n",
        "            ¡Hola! Soy tu asistente basado en documentos. Hazme preguntas sobre el Proyecto 'Titán'.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        # El ChatInterface ya incluye su propia caja de texto para entrada\n",
        "        chatbot_ui = gr.ChatInterface(\n",
        "            fn=respond,\n",
        "            title=\" \",  # Título ya lo pusimos en Markdown, o puedes ponerlo aquí\n",
        "            description=\" \",  # Descripción ya la pusimos en Markdown, o puedes ponerla aquí\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            exportar_btn = gr.Button(\"📁 Exportar Historial de Chat\")\n",
        "            # Este Textbox ya no necesita ser actualizado directamente por el botón,\n",
        "            # lo mantenemos por si acaso o para otros usos, pero no lo actualizaremos con el toast\n",
        "            # gr.Info se encarga de eso.\n",
        "            export_result_display = gr.Textbox(\n",
        "                label=\"Estado de Exportación\", interactive=False, visible=False\n",
        "            )\n",
        "\n",
        "        # El botón de exportar llama a la función que dispara el toast\n",
        "        exportar_btn.click(fn=export_chat_action, outputs=None)\n",
        "\n",
        "    return demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ruta a los documentos\n",
        "document_paths = [\"./documents\"]\n",
        "\n",
        "new_chatbot = RAGChatbot(\n",
        "    model_name=\"gemma3:1b\",\n",
        "    embedding_model=\"gemma2:2b\",\n",
        "    db_type=\"faiss\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        ")\n",
        "\n",
        "# Configurar el sistema completo\n",
        "new_chatbot.setup(document_paths=document_paths)\n",
        "\n",
        "demo = gradio_interface(new_chatbot)\n",
        "demo.launch(share=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx-dZSFJz9cK"
      },
      "source": [
        "# **Conclusiones:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w3usdaC0BCj"
      },
      "source": [
        "* #### **Incluyan sus conclusiones de la actividad chatbot LLM + RAG:**\n",
        "\n",
        "\n",
        "\n",
        "None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtB5Q3m41YQ0"
      },
      "source": [
        "# **Fin de la actividad chatbot: LLM + RAG**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
